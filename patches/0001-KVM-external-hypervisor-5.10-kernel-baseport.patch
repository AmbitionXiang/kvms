From 54772461f1fbef4ecaa4f2372d47e6160152b942 Mon Sep 17 00:00:00 2001
From: Janne Karhunen <Janne.Karhunen@gmail.com>
Date: Tue, 21 Apr 2020 09:45:23 +0300
Subject: [PATCH] KVM: external hypervisor 5.10 kernel baseport

Signed-off-by: Janne Karhunen <Janne.Karhunen@gmail.com>
Signed-off-by: Jani Hyvonen <jani.hyvonen@digital14.com>
---
 arch/arm64/configs/defconfig      |  10 +
 arch/arm64/include/asm/kvm_asm.h  |  24 ++
 arch/arm64/include/asm/kvm_host.h |  19 +-
 arch/arm64/include/asm/kvm_hyp.h  |  13 ++
 arch/arm64/include/asm/kvm_mmu.h  |  67 ++++--
 arch/arm64/kernel/asm-offsets.c   |   7 +
 arch/arm64/kernel/head.S          |  16 +-
 arch/arm64/kvm/Makefile           |   2 +-
 arch/arm64/kvm/arm.c              |  33 ++-
 arch/arm64/kvm/ext-guest.c        | 360 ++++++++++++++++++++++++++++++
 arch/arm64/kvm/ext-guest.h        |  21 ++
 arch/arm64/kvm/hvccall-defines.h  |  57 +++++
 arch/arm64/kvm/hyp/Makefile       |   2 +-
 arch/arm64/kvm/hyp/entry.S        |  13 +-
 arch/arm64/kvm/hyp/kvms-hvci.S    |  11 +
 arch/arm64/kvm/hyp/nvhe/switch.c  |  15 +-
 arch/arm64/kvm/hyp/nvhe/tlb.c     |  11 +-
 arch/arm64/kvm/hyp/pgtable.c      |  24 +-
 arch/arm64/kvm/hyp/vgic-v3-sr.c   |   6 +
 arch/arm64/kvm/mmu.c              |  94 +++++---
 arch/arm64/kvm/reset.c            |   7 +-
 arch/arm64/lib/copy_template.S    |   9 +
 22 files changed, 720 insertions(+), 101 deletions(-)
 create mode 100644 arch/arm64/kvm/ext-guest.c
 create mode 100644 arch/arm64/kvm/ext-guest.h
 create mode 100644 arch/arm64/kvm/hvccall-defines.h
 create mode 100644 arch/arm64/kvm/hyp/kvms-hvci.S

diff --git a/arch/arm64/configs/defconfig b/arch/arm64/configs/defconfig
index 5cfe3cf6f2ac..fa9bd1e9608f 100644
--- a/arch/arm64/configs/defconfig
+++ b/arch/arm64/configs/defconfig
@@ -1092,3 +1092,13 @@ CONFIG_DEBUG_KERNEL=y
 # CONFIG_DEBUG_PREEMPT is not set
 # CONFIG_FTRACE is not set
 CONFIG_MEMTEST=y
+CONFIG_KVM_GUEST=y
+CONFIG_VIRTIO_INPUT=y
+CONFIG_VIRTIO_MMIO_CMDLINE_DEVICES=y
+CONFIG_DRM_VIRTIO_GPU=y
+CONFIG_HW_RANDOM_VIRTIO=y
+CONFIG_VIRTIO_IOMMU=y
+CONFIG_VIRTIO_PMEM=y
+CONFIG_VIRTIO_VSOCKETS_COMMON=y
+CONFIG_VIRTIO_VSOCKETS=y
+CONFIG_ARM64_PTR_AUTH=n
diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 044bb9e2cd74..db5eec9c4ebc 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -180,23 +180,47 @@ DECLARE_KVM_HYP_SYM(__bp_harden_hyp_vecs);
 #define __bp_harden_hyp_vecs	CHOOSE_HYP_SYM(__bp_harden_hyp_vecs)
 
 extern void __kvm_flush_vm_context(void);
+DECLARE_KVM_NVHE_SYM(__kvm_flush_vm_context);
+
 extern void __kvm_flush_cpu_context(struct kvm_s2_mmu *mmu);
+DECLARE_KVM_NVHE_SYM(__kvm_flush_cpu_context);
+
 extern void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa,
 				     int level);
+DECLARE_KVM_NVHE_SYM(__kvm_tlb_flush_vmid_ipa);
+
 extern void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu);
+DECLARE_KVM_NVHE_SYM(__kvm_tlb_flush_vmid);
+
+extern void __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu);
+DECLARE_KVM_NVHE_SYM(__kvm_tlb_flush_local_vmid);
 
 extern void __kvm_timer_set_cntvoff(u64 cntvoff);
+DECLARE_KVM_NVHE_SYM(__kvm_timer_set_cntvoff);
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
+DECLARE_KVM_NVHE_SYM(__kvm_vcpu_run);
 
 extern void __kvm_enable_ssbs(void);
+DECLARE_KVM_NVHE_SYM(__kvm_enable_ssbs);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_get_ich_vtr_el2);
+
 extern u64 __vgic_v3_read_vmcr(void);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_read_vmcr);
+
 extern void __vgic_v3_write_vmcr(u32 vmcr);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_write_vmcr);
+
 extern void __vgic_v3_init_lrs(void);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_init_lrs);
 
 extern u32 __kvm_get_mdcr_el2(void);
+DECLARE_KVM_NVHE_SYM(__kvm_get_mdcr_el2);
+
+int __guest_exit(void);
+DECLARE_KVM_NVHE_SYM(__guest_exit);
 
 extern char __smccc_workaround_1_smc[__SMCCC_WORKAROUND_1_SMC_SZ];
 
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index cc060c41adaa..b243bcc2831d 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -87,6 +87,8 @@ struct kvm_s2_mmu {
 	int __percpu *last_vcpu_ran;
 
 	struct kvm *kvm;
+	u64 host_vttbr;
+	u64 host_vtcr;
 };
 
 struct kvm_arch {
@@ -262,6 +264,7 @@ struct kvm_cpu_context {
 	};
 
 	struct kvm_vcpu *__hyp_running_vcpu;
+	u64 *host_sp;
 };
 
 struct kvm_pmu_events {
@@ -489,8 +492,10 @@ int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 
 void kvm_arm_halt_guest(struct kvm *kvm);
 void kvm_arm_resume_guest(struct kvm *kvm);
+int __kvm_call_hyp_reg(void *, ...);
 
-#define kvm_call_hyp_nvhe(f, ...)						\
+#if 0
+#define kvm_call_hyp_nvhe(f, ...)					\
 	({								\
 		struct arm_smccc_res res;				\
 									\
@@ -500,7 +505,17 @@ void kvm_arm_resume_guest(struct kvm *kvm);
 									\
 		res.a1;							\
 	})
-
+#else
+#define kvm_call_hyp_nvhe(f, ...)					\
+	({								\
+		int ret;                                                \
+									\
+		ret = __kvm_call_hyp_reg(kvm_ksym_ref_nvhe(f),		\
+					 ##__VA_ARGS__);		\
+									\
+		ret;							\
+})
+#endif
 /*
  * The couple of isb() below are there to guarantee the same behaviour
  * on VHE as on !VHE, where the eret to EL1 acts as a context
diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 123e67cb8505..fa057c50ee6f 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -57,12 +57,25 @@ DECLARE_PER_CPU(unsigned long, kvm_hyp_vector);
 int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_save_state);
+
 void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_restore_state);
+
 void __vgic_v3_activate_traps(struct vgic_v3_cpu_if *cpu_if);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_activate_traps);
+
 void __vgic_v3_deactivate_traps(struct vgic_v3_cpu_if *cpu_if);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_deactivate_traps);
+
 void __vgic_v3_save_aprs(struct vgic_v3_cpu_if *cpu_if);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_save_aprs);
+
 void __vgic_v3_restore_aprs(struct vgic_v3_cpu_if *cpu_if);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_restore_aprs);
+
 int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu);
+DECLARE_KVM_NVHE_SYM(__vgic_v3_perform_cpuif_access);
 
 #ifdef __KVM_NVHE_HYPERVISOR__
 void __timer_enable_traps(struct kvm_vcpu *vcpu);
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 331394306cce..62290e9cb8fd 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -49,6 +49,8 @@
  * mappings, and none of this applies in that case.
  */
 
+#include "../../kvm/hvccall-defines.h"
+
 #ifdef __ASSEMBLY__
 
 #include <asm/alternative.h>
@@ -62,15 +64,10 @@
  * perform the register allocation (kvm_update_va_mask uses the
  * specific registers encoded in the instructions).
  */
-.macro kern_hyp_va	reg
-alternative_cb kvm_update_va_mask
-	and     \reg, \reg, #1		/* mask with va_mask */
-	ror	\reg, \reg, #1		/* rotate to the first tag bit */
-	add	\reg, \reg, #0		/* insert the low 12 bits of the tag */
-	add	\reg, \reg, #0, lsl 12	/* insert the top 12 bits of the tag */
-	ror	\reg, \reg, #63		/* rotate back */
-alternative_cb_end
-.endm
+.macro kern_hyp_va      reg
+	and	\reg, \reg, #CALL_MASK
+	orr	\reg, \reg, #KERNEL_BASE
+ .endm
 
 #else
 
@@ -86,13 +83,9 @@ void kvm_compute_layout(void);
 
 static __always_inline unsigned long __kern_hyp_va(unsigned long v)
 {
-	asm volatile(ALTERNATIVE_CB("and %0, %0, #1\n"
-				    "ror %0, %0, #1\n"
-				    "add %0, %0, #0\n"
-				    "add %0, %0, #0, lsl 12\n"
-				    "ror %0, %0, #63\n",
-				    kvm_update_va_mask)
-		     : "+r" (v));
+	v = (v & ~KERNEL_MAP);
+	v = KERNEL_BASE | v;
+
 	return v;
 }
 
@@ -110,8 +103,28 @@ static __always_inline unsigned long __kern_hyp_va(unsigned long v)
 
 #include <asm/kvm_pgtable.h>
 #include <asm/stage2_pgtable.h>
-
+#include <linux/kvm_types.h>
+
+struct hyp_map_data {
+	u64             phys;
+	kvm_pte_t       attr;
+};
+
+struct stage2_map_data {
+	u64				phys;
+	kvm_pte_t			attr;
+	kvm_pte_t			*anchor;
+	struct kvm_s2_mmu		*mmu;
+	struct kvm_mmu_memory_cache	*memcache;
+};
+
+int kvm_set_hyp_text(void *ts, void *te);
+int hyp_map_set_prot_attr(enum kvm_pgtable_prot prot,
+			  struct hyp_map_data *data);
+int stage2_map_set_prot_attr(enum kvm_pgtable_prot prot,
+			     struct stage2_map_data *data);
 int create_hyp_mappings(void *from, void *to, enum kvm_pgtable_prot prot);
+int clean_hyp_mappings(void *from, void *to);
 int create_hyp_io_mappings(phys_addr_t phys_addr, size_t size,
 			   void __iomem **kaddr,
 			   void __iomem **haddr);
@@ -131,8 +144,6 @@ phys_addr_t kvm_mmu_get_httbr(void);
 phys_addr_t kvm_get_idmap_vector(void);
 int kvm_mmu_init(void);
 
-struct kvm;
-
 #define kvm_flush_dcache_to_poc(a,l)	__flush_dcache_area((a), (l))
 
 static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)
@@ -273,8 +284,14 @@ static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
  */
 static __always_inline void __load_guest_stage2(struct kvm_s2_mmu *mmu)
 {
-	write_sysreg(kern_hyp_va(mmu->kvm)->arch.vtcr, vtcr_el2);
-	write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	struct kvm_s2_mmu *kmmu = kern_hyp_va(mmu);
+	struct kvm *kvm = kern_hyp_va(kmmu->kvm);
+
+	kmmu->host_vttbr = read_sysreg(vttbr_el2);
+	kmmu->host_vtcr = read_sysreg(vtcr_el2);
+
+	write_sysreg(kvm->arch.vtcr, vtcr_el2);
+	write_sysreg(kvm_get_vttbr(kmmu), vttbr_el2);
 
 	/*
 	 * ARM errata 1165522 and 1530923 require the actual execution of the
@@ -284,5 +301,13 @@ static __always_inline void __load_guest_stage2(struct kvm_s2_mmu *mmu)
 	asm(ALTERNATIVE("nop", "isb", ARM64_WORKAROUND_SPECULATIVE_AT));
 }
 
+static __always_inline void __load_host_stage2_(struct kvm_s2_mmu *mmu)
+{
+	struct kvm_s2_mmu *kmmu = kern_hyp_va(mmu);
+
+	write_sysreg(kmmu->host_vttbr, VTTBR_EL2);
+	write_sysreg(kmmu->host_vtcr, VTCR_EL2);
+}
+
 #endif /* __ASSEMBLY__ */
 #endif /* __ARM64_KVM_MMU_H__ */
diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
index 7d32fc959b1a..78df08e2de3e 100644
--- a/arch/arm64/kernel/asm-offsets.c
+++ b/arch/arm64/kernel/asm-offsets.c
@@ -110,6 +110,13 @@ int main(void)
   DEFINE(CPU_APGAKEYLO_EL1,	offsetof(struct kvm_cpu_context, sys_regs[APGAKEYLO_EL1]));
   DEFINE(HOST_CONTEXT_VCPU,	offsetof(struct kvm_cpu_context, __hyp_running_vcpu));
   DEFINE(HOST_DATA_CONTEXT,	offsetof(struct kvm_host_data, host_ctxt));
+  DEFINE(CPU_HOST_SP,		offsetof(struct kvm_cpu_context, host_sp));
+  DEFINE(KVM_ARCH,		offsetof(struct kvm, arch));
+  DEFINE(KVM_ARCH_MMU,		offsetof(struct kvm_arch, mmu));
+  DEFINE(KVM_S2MMU_VMID,	offsetof(struct kvm_s2_mmu, vmid));
+  DEFINE(KVM_S2MMU_PGD,		offsetof(struct kvm_s2_mmu, pgd_phys));
+  DEFINE(KVM_ARCH_PGD,		offsetof(struct kvm_arch, mmu) + offsetof(struct kvm_s2_mmu, pgd_phys));
+  DEFINE(KVM_ARCH_VMID,		offsetof(struct kvm_arch, mmu) + offsetof(struct kvm_s2_mmu, vmid));
 #endif
 #ifdef CONFIG_CPU_PM
   DEFINE(CPU_CTX_SP,		offsetof(struct cpu_suspend_ctx, sp));
diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
index 78cdd6b24172..0de3d10180a4 100644
--- a/arch/arm64/kernel/head.S
+++ b/arch/arm64/kernel/head.S
@@ -489,7 +489,21 @@ EXPORT_SYMBOL(kimage_vaddr)
  * booted in EL1 or EL2 respectively.
  */
 SYM_FUNC_START(el2_setup)
-	msr	SPsel, #1			// We want to use SP_EL{1,2}
+	/*
+	 * Jump to the hypervisor and fake we did el2 setup here.
+	 */
+	mov	x26, lr
+	ldr	x27, =0x100000000
+	ldr	x28, =0x48000000
+	adr	x30, el2_setup
+	add	x30, x30, 24
+	br	x27
+
+	msr	SPsel, #1			// We want to use SP_EL{1,2}a
+	mov	w0, #BOOT_CPU_MODE_EL2          // This CPU booted in EL2
+	isb
+	ret
+
 	mrs	x0, CurrentEL
 	cmp	x0, #CurrentEL_EL2
 	b.eq	1f
diff --git a/arch/arm64/kvm/Makefile b/arch/arm64/kvm/Makefile
index 1504c81fbf5d..66558ce2f53b 100644
--- a/arch/arm64/kvm/Makefile
+++ b/arch/arm64/kvm/Makefile
@@ -22,6 +22,6 @@ kvm-y := $(KVM)/kvm_main.o $(KVM)/coalesced_mmio.o $(KVM)/eventfd.o \
 	 vgic/vgic-v3.o vgic/vgic-v4.o \
 	 vgic/vgic-mmio.o vgic/vgic-mmio-v2.o \
 	 vgic/vgic-mmio-v3.o vgic/vgic-kvm-device.o \
-	 vgic/vgic-its.o vgic/vgic-debug.o
+	 vgic/vgic-its.o vgic/vgic-debug.o ext-guest.o
 
 kvm-$(CONFIG_KVM_ARM_PMU)  += pmu-emul.o
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index a1c2c955474e..375c256677bc 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -37,10 +37,12 @@
 #include <asm/kvm_emulate.h>
 #include <asm/kvm_coproc.h>
 #include <asm/sections.h>
+#include <asm/kvm_host.h>
 
 #include <kvm/arm_hypercalls.h>
 #include <kvm/arm_pmu.h>
 #include <kvm/arm_psci.h>
+#include "ext-guest.h"
 
 #ifdef REQUIRES_VIRT
 __asm__(".arch_extension	virt");
@@ -55,6 +57,7 @@ unsigned long kvm_arm_hyp_percpu_base[NR_CPUS];
 static atomic64_t kvm_vmid_gen = ATOMIC64_INIT(1);
 static u32 kvm_next_vmid;
 static DEFINE_SPINLOCK(kvm_vmid_lock);
+const u64 hypmode = 1;
 
 static bool vgic_present;
 
@@ -132,9 +135,11 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (ret)
 		return ret;
 
-	ret = create_hyp_mappings(kvm, kvm + 1, PAGE_HYP);
-	if (ret)
-		goto out_free_stage2_pgd;
+	ret = kvm_init_guest(kvm);
+	if (ret) {
+		kvm_free_guest(kvm);
+		return ret;
+	}
 
 	kvm_vgic_early_init(kvm);
 
@@ -143,9 +148,6 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	set_default_csv2(kvm);
 
-	return ret;
-out_free_stage2_pgd:
-	kvm_free_stage2_pgd(&kvm->arch.mmu);
 	return ret;
 }
 
@@ -154,7 +156,6 @@ vm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
 	return VM_FAULT_SIGBUS;
 }
 
-
 /**
  * kvm_arch_destroy_vm - destroy the VM data structure
  * @kvm:	pointer to the KVM struct
@@ -252,6 +253,8 @@ struct kvm *kvm_arch_alloc_vm(void)
 
 void kvm_arch_free_vm(struct kvm *kvm)
 {
+	kvm_free_guest(kvm);
+
 	if (!has_vhe())
 		kfree(kvm);
 	else
@@ -303,6 +306,10 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
+	u8 *start = (u8 *)vcpu;
+	u8 *end = start + sizeof(*vcpu);
+
+	clean_hyp_mappings((void *)start, (void *)end);
 	if (vcpu->arch.has_run_once && unlikely(!irqchip_in_kernel(vcpu->kvm)))
 		static_branch_dec(&userspace_irqchip_in_use);
 
@@ -1358,6 +1365,11 @@ static void cpu_init_hyp_mode(void)
 	unsigned long tpidr_el2;
 	struct arm_smccc_res res;
 
+	if (hypmode != 0) {
+		ext_hyp_cpu_init();
+		return;
+	}
+
 	/* Switch from the HYP stub to our own HYP init vector */
 	__hyp_set_vectors(kvm_get_idmap_vector());
 
@@ -1397,8 +1409,6 @@ static void cpu_init_hyp_mode(void)
 
 static void cpu_hyp_reset(void)
 {
-	if (!is_kernel_in_hyp_mode())
-		__hyp_reset_vectors();
 }
 
 static void cpu_hyp_reinit(void)
@@ -1781,7 +1791,10 @@ int kvm_arch_init(void *opaque)
 		return err;
 
 	if (!in_hyp_mode) {
-		err = init_hyp_mode();
+		if (hypmode != 1)
+			err = init_hyp_mode();
+		else
+			err = ext_hyp_init();
 		if (err)
 			goto out_err;
 	}
diff --git a/arch/arm64/kvm/ext-guest.c b/arch/arm64/kvm/ext-guest.c
new file mode 100644
index 000000000000..cdd212a7993b
--- /dev/null
+++ b/arch/arm64/kvm/ext-guest.c
@@ -0,0 +1,360 @@
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/kvm_host.h>
+#include <linux/io.h>
+#include <linux/hugetlb.h>
+#include <linux/sched/signal.h>
+#include <trace/events/kvm.h>
+#include <asm/pgalloc.h>
+#include <asm/cacheflush.h>
+#include <asm/kvm_arm.h>
+#include <asm/kvm_mmu.h>
+#include <asm/kvm_pgtable.h>
+#include <asm/kvm_ras.h>
+#include <asm/kvm_asm.h>
+#include <asm/kvm_emulate.h>
+#include <asm/virt.h>
+
+#include "hvccall-defines.h"
+
+static unsigned long nvhe_percpu_size(void)
+{
+	return (unsigned long)CHOOSE_NVHE_SYM(__per_cpu_end) -
+		(unsigned long)CHOOSE_NVHE_SYM(__per_cpu_start);
+}
+
+static unsigned long nvhe_percpu_order(void)
+{
+	unsigned long size = nvhe_percpu_size();
+
+	return size ? get_order(size) : 0;
+}
+
+int create_guest_mapping(u32 vmid, unsigned long start, unsigned long phys,
+			 unsigned long size, u64 prot)
+{
+	int err;
+
+	start = PAGE_ALIGN(start);
+	phys = PAGE_ALIGN(phys);
+	size = PAGE_ALIGN(size);
+
+	err = __kvms_hvc_cmd(HYP_GUEST_MAP_STAGE2, vmid, start, phys, size,
+			     prot);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+
+	return err;
+}
+
+void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
+{
+	u32 vmid = vcpu->kvm->arch.mmu.vmid.vmid;
+	int err;
+
+	err = __kvms_hvc_cmd(HYP_MKYOUNG, vmid, fault_ipa);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+}
+
+int update_hyp_memslots(struct kvm *kvm, struct kvm_memory_slot *slot,
+			const struct kvm_userspace_memory_region *mem)
+{
+	int err;
+
+	err = __kvms_hvc_cmd(HYP_UPDATE_GUEST_MEMSLOT, kvm, slot, mem);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+
+	return err;
+}
+
+static void __unmap_stage2_range(struct kvm_s2_mmu *mmu,
+				 phys_addr_t start,
+				 u64 size,
+				 u64 measure)
+{
+	u32 vmid = mmu->vmid.vmid;
+	u64 npages;
+	int err;
+
+	err = __kvms_hvc_cmd(HYP_GUEST_UNMAP_STAGE2, vmid, start,
+			     size, measure);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+
+	/*
+	 * If the hyp returned an error it did not fully unmap the
+	 * blob, either the section was not completely mapped OR we
+	 * hit a dirty page.
+	 */
+	if ((err & 0xFFFF) == 0xF0F0) {
+		npages = (size / PAGE_SIZE) - (err >> 16);
+		if (npages) {
+			pr_warn("Unmap request %p/%llu fell short \
+				%llu pages\n", (void *)start, size,
+				npages);
+		}
+	}
+}
+
+void unmap_stage2_range_sec(struct kvm_s2_mmu *mmu,
+			    phys_addr_t start, u64 size)
+{
+	__unmap_stage2_range(mmu, start, size, 1);
+}
+
+void unmap_stage2_range(struct kvm_s2_mmu *mmu,
+			phys_addr_t start, u64 size)
+{
+	__unmap_stage2_range(mmu, start, size, 0);
+}
+
+int __create_hyp_mappings(unsigned long start, unsigned long size,
+			  unsigned long phys, enum kvm_pgtable_prot prot)
+{
+	struct hyp_map_data data = {
+		.phys   = ALIGN_DOWN(phys, PAGE_SIZE),
+	};
+	int err;
+
+	err = hyp_map_set_prot_attr(prot, &data);
+	if (err)
+		return err;
+
+	err = __kvms_hvc_cmd(HYP_HOST_MAP_STAGE1, start, phys, size, prot);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+
+	return err;
+}
+
+static int __clean_hyp_mappings(void *start, size_t size)
+{
+	int err;
+
+	err = __kvms_hvc_cmd(HYP_HOST_UNMAP_STAGE1, start, size);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+
+	return err;
+}
+
+int clean_hyp_mappings(void *from, void *to)
+{
+	unsigned long start;
+	size_t size;
+
+	from  = (void *)ALIGN_DOWN((u64)from, PAGE_SIZE);
+	to = (void *)ALIGN((u64)to, PAGE_SIZE);
+	size = to - from;
+	start = kern_hyp_va((unsigned long)from);
+
+	return  __clean_hyp_mappings((void *)start, size);
+}
+
+int kvm_set_hyp_text(void *__ts, void *__te)
+{
+	unsigned long start = kern_hyp_va((unsigned long)__ts);
+	unsigned long end = kern_hyp_va((unsigned long)__te);
+	unsigned long *guest_exit = kvm_ksym_ref(__guest_exit);
+	int err;
+
+	err = __kvms_hvc_cmd(HYP_SET_HYP_TXT, start, end,
+			     guest_exit, NULL);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+
+	return err;
+}
+
+static int __kvm_init_guest(struct kvm *kvm)
+{
+	int err;
+
+	err = __kvms_hvc_cmd(HYP_INIT_GUEST, kvm);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+
+	return err;
+}
+
+int kvm_init_guest(struct kvm *kvm)
+{
+	u8 *kvmstart = (u8 *)kvm;
+	u8 *kvmend = kvmstart + sizeof(*kvm);
+	u8 *mmustart = (u8 *)&kvm->arch.mmu;
+	u8 *mmuend = mmustart + sizeof(kvm->arch.mmu);
+	int ret;
+
+	ret = create_hyp_mappings(kvmstart, kvmend, PAGE_HYP);
+	if (ret)
+		return ret;
+
+	ret = create_hyp_mappings(mmustart, mmuend, PAGE_HYP);
+	if (ret) {
+		clean_hyp_mappings(kvmstart, kvmend);
+		return ret;
+	}
+
+	ret = __kvm_init_guest(kvm);
+	if (ret) {
+		clean_hyp_mappings(kvmstart, kvmend);
+		clean_hyp_mappings(mmustart, mmuend);
+		kvm_err("kvm_init_guest returned %d\n", ret);
+	}
+
+	return ret;
+}
+
+
+static int __kvm_free_guest(struct kvm *kvm)
+{
+	int err;
+
+	err = __kvms_hvc_cmd(HYP_FREE_GUEST, kvm);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+
+	return err;
+}
+
+int kvm_free_guest(struct kvm *kvm)
+{
+	u8 *kvmstart = (u8 *)kvm;
+	u8 *kvmend = kvmstart + sizeof(*kvm);
+	u8 *mmustart = (u8 *)&kvm->arch.mmu;
+	u8 *mmuend = mmustart + sizeof(kvm->arch.mmu);
+
+	clean_hyp_mappings(kvmstart, kvmend);
+	clean_hyp_mappings(mmustart, mmuend);
+
+	return __kvm_free_guest(kvm);
+}
+
+noinline
+int __kvm_call_hyp_reg(void *a0, ...)
+{
+	register int reg0 asm ("x0");
+	register u64 reg1 asm ("x1");
+	register u64 reg2 asm ("x2");
+	register u64 reg3 asm ("x3");
+	register u64 reg4 asm ("x4");
+	register u64 reg5 asm ("x5");
+	register u64 reg6 asm ("x6");
+	register u64 reg7 asm ("x7");
+	register u64 reg8 asm ("x8");
+	register u64 reg9 asm ("x9");
+
+	__asm__ __volatile__ (
+		"hvc	#0"
+		: "=r"(reg0)
+		: [reg0]"r"(reg0), [reg1]"r"(reg1), [reg2]"r"(reg2),
+		  [reg3]"r"(reg3), [reg4]"r"(reg4), [reg5]"r"(reg5),
+		  [reg6]"r"(reg6), [reg7]"r"(reg7), [reg8]"r"(reg8),
+		  [reg9]"r"(reg9)
+		: "memory");
+
+	return reg0;
+}
+
+void ext_hyp_cpu_init(void)
+{
+	unsigned long id;
+	unsigned long sym;
+	unsigned long tpidr_el2;
+	int err;
+
+	id = smp_processor_id();
+	sym = (unsigned long)kvm_ksym_ref(CHOOSE_NVHE_SYM(__per_cpu_start));
+	tpidr_el2 = (unsigned long)this_cpu_ptr_nvhe_sym(__per_cpu_start) -
+		    sym;
+
+	err = __kvms_hvc_cmd(HYP_SET_TPIDR, tpidr_el2, id, sym);
+	if (err)
+		pr_err("kvm: %s failed: %d\n", __func__, err);
+}
+
+int ext_hyp_init(void)
+{
+	int cpu;
+	int err = 0;
+
+	/*
+	 * Tell the external hypervisor where the kernel el2 code
+	 * is in the memory.
+	 */
+	err = kvm_set_hyp_text(kvm_ksym_ref(__hyp_text_start),
+			       kvm_ksym_ref(__hyp_text_end));
+	if (err) {
+		kvm_err("Cannot set hyp txt\n");
+		goto out_err;
+	}
+	/*
+	 * Allocate and initialize pages for Hypervisor-mode percpu regions.
+	 */
+	for_each_possible_cpu(cpu) {
+		struct page *page;
+		void *page_addr;
+
+		page = alloc_pages(GFP_KERNEL, nvhe_percpu_order());
+		if (!page) {
+			err = -ENOMEM;
+			goto out_err;
+		}
+
+		page_addr = page_address(page);
+		memcpy(page_addr, CHOOSE_NVHE_SYM(__per_cpu_start), nvhe_percpu_size());
+		kvm_arm_hyp_percpu_base[cpu] = (unsigned long)page_addr;
+	}
+	/*
+	 * Map the Hyp-code called directly from the host
+	 */
+	err = create_hyp_mappings(kvm_ksym_ref(__hyp_text_start),
+				  kvm_ksym_ref(__hyp_text_end), PAGE_HYP_EXEC);
+	if (err) {
+		kvm_err("Cannot map world-switch code\n");
+		goto out_err;
+	}
+
+	err = create_hyp_mappings(kvm_ksym_ref(__start_rodata),
+				  kvm_ksym_ref(__end_rodata), PAGE_HYP_RO);
+	if (err) {
+		kvm_err("Cannot map rodata section\n");
+		goto out_err;
+	}
+
+	err = create_hyp_mappings(kvm_ksym_ref(__bss_start),
+				  kvm_ksym_ref(__bss_stop), PAGE_HYP_RO);
+	if (err) {
+		kvm_err("Cannot map bss section\n");
+		goto out_err;
+	}
+	/*
+	 * Map Hyp percpu pages
+	 */
+	for_each_possible_cpu(cpu) {
+		char *percpu_begin = (char *)kvm_arm_hyp_percpu_base[cpu];
+		char *percpu_end = percpu_begin + nvhe_percpu_size();
+
+		err = create_hyp_mappings(percpu_begin, percpu_end, PAGE_HYP);
+
+		if (err) {
+			kvm_err("Cannot map hyp percpu region\n");
+			goto out_err;
+		}
+	}
+	kvm_info("Hyp mode initialized successfully\n");
+	return 0;
+
+out_err:
+	clean_hyp_mappings(kvm_ksym_ref(__hyp_text_start),
+			   kvm_ksym_ref(__hyp_text_end));
+	clean_hyp_mappings(kvm_ksym_ref(__start_rodata),
+			   kvm_ksym_ref(__end_rodata));
+	clean_hyp_mappings(kvm_ksym_ref(__bss_start),
+			   kvm_ksym_ref(__bss_stop));
+
+	kvm_err("error initializing Hyp mode: %d\n", err);
+	return err;
+}
diff --git a/arch/arm64/kvm/ext-guest.h b/arch/arm64/kvm/ext-guest.h
new file mode 100644
index 000000000000..c727772cc7b0
--- /dev/null
+++ b/arch/arm64/kvm/ext-guest.h
@@ -0,0 +1,21 @@
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <asm/kvm_asm.h>
+
+int ext_hyp_init(void);
+void ext_hyp_cpu_init(void);
+int __kvm_call_hyp_reg(void *a0, ...);
+int kvm_set_hyp_text(void *__ts, void *__te);
+int kvm_init_guest(struct kvm *kvm);
+int kvm_free_guest(struct kvm *kvm);
+
+void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa);
+int __create_hyp_mappings(unsigned long start, unsigned long size,
+			  unsigned long phys, enum kvm_pgtable_prot prot);
+int clean_hyp_mappings(void *from, void *to);
+int create_guest_mapping(u32 vmid, unsigned long start, unsigned long phys,
+			unsigned long size, u64 prot);
+int update_hyp_memslots(struct kvm *kvm, struct kvm_memory_slot *slot,
+			const struct kvm_userspace_memory_region *mem);
+void unmap_stage2_range(struct kvm_s2_mmu *mmu, phys_addr_t start, u64 size);
+void unmap_stage2_range_sec(struct kvm_s2_mmu *mmu, phys_addr_t start, u64 size);
diff --git a/arch/arm64/kvm/hvccall-defines.h b/arch/arm64/kvm/hvccall-defines.h
new file mode 100644
index 000000000000..40a3a2064076
--- /dev/null
+++ b/arch/arm64/kvm/hvccall-defines.h
@@ -0,0 +1,57 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+#ifndef __HYP_API__
+#define __HYP_API__
+/*
+ * Base addressing for data sharing
+ */
+#define KERNEL_MAP	0xFFFFFFF000000000
+#define KERN_VA_MASK	0x0000000FFFFFFFFF
+#define CALL_MASK	KERN_VA_MASK
+#define KERNEL_BASE	0x4000000000
+
+/*
+ * Host protection support
+ */
+#define HYP_FIRST_HOSTCALL		0x8000
+#define HYP_HOST_MAP_STAGE1		0x8000
+#define HYP_HOST_MAP_STAGE2		0x8001
+#define HYP_HOST_UNMAP_STAGE1		0x8002
+#define HYP_HOST_UNMAP_STAGE2		0x8003
+#define HYP_HOST_BOOTSTEP		0x8004
+#define HYP_HOST_GET_VMID		0x8005
+#define HYP_HOST_SET_LOCKFLAGS		0x8006
+#define HYP_LAST_HOSTCALL		HYP_HOST_SET_LOCKFLAGS
+
+/*
+ * KVM guest support
+ */
+#define HYP_FIRST_GUESTCALL		0x9000
+#define HYP_READ_MDCR_EL2		0x9000
+#define HYP_SET_HYP_TXT			0x9001
+#define HYP_SET_TPIDR			0x9002
+#define HYP_INIT_GUEST			0x9003
+#define HYP_FREE_GUEST			0x9004
+#define HYP_UPDATE_GUEST_MEMSLOT	0x9005
+#define HYP_GUEST_MAP_STAGE2		0x9006
+#define HYP_GUEST_UNMAP_STAGE2		0x9007
+#define HYP_SET_WORKMEM			0x9008
+#define HYP_USER_COPY			0x9009
+#define HYP_MKYOUNG			0x900A
+#define HYP_SET_GUEST_MEMORY_OPEN	0x900B
+#define HYP_SET_GUEST_MEMORY_BLINDED	0x900C
+#define HYP_LAST_GUESTCALL		HYP_SET_GUEST_MEMORY_BLINDED
+
+/*
+ * Misc
+ */
+#define HYP_READ_LOG			0xA000
+
+#define STR(x) #x
+#define XSTR(s) STR(s)
+
+#ifndef __ASSEMBLY__
+extern int __kvms_hvc_cmd(unsigned long cmd, ...);
+extern uint64_t __kvms_hvc_get(unsigned long cmd, ...);
+#endif // __ASSEMBLY__
+
+#endif // __HYP_API__
diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
index 4a81eddabcd8..d9bde57dc08c 100644
--- a/arch/arm64/kvm/hyp/Makefile
+++ b/arch/arm64/kvm/hyp/Makefile
@@ -10,4 +10,4 @@ subdir-ccflags-y := -I$(incdir)				\
 		    -DDISABLE_BRANCH_PROFILING		\
 		    $(DISABLE_STACKLEAK_PLUGIN)
 
-obj-$(CONFIG_KVM) += vhe/ nvhe/ pgtable.o smccc_wa.o
+obj-$(CONFIG_KVM) += vhe/ nvhe/ pgtable.o smccc_wa.o kvms-hvci.o
diff --git a/arch/arm64/kvm/hyp/entry.S b/arch/arm64/kvm/hyp/entry.S
index 0c66a1d408fd..129c75a7e1b0 100644
--- a/arch/arm64/kvm/hyp/entry.S
+++ b/arch/arm64/kvm/hyp/entry.S
@@ -30,6 +30,10 @@ SYM_FUNC_START(__guest_enter)
 	// Store the hyp regs
 	save_callee_saved_regs x1
 
+	// Save host sp
+	mov	x2, sp
+	str	x2, [x1, #CPU_HOST_SP]
+
 	// Save hyp's sp_el0
 	save_sp_el0	x1, x2
 
@@ -102,14 +106,16 @@ SYM_INNER_LABEL(__guest_exit_panic, SYM_L_GLOBAL)
 SYM_INNER_LABEL(__guest_exit, SYM_L_GLOBAL)
 	// x0: return code
 	// x1: vcpu
-	// x2-x29,lr: vcpu regs
-	// vcpu x0-x1 on the stack
+	// x2-3: clobbered
+	// x4-x29,lr: vcpu regs
+	// vcpu x0-x4 on the stack
 
 	add	x1, x1, #VCPU_CONTEXT
 
 	ALTERNATIVE(nop, SET_PSTATE_PAN(1), ARM64_HAS_PAN, CONFIG_ARM64_PAN)
 
 	// Store the guest regs x2 and x3
+	ldp	x2, x3,	[sp, #(8 * 2)]
 	stp	x2, x3,   [x1, #CPU_XREG_OFFSET(2)]
 
 	// Retrieve the guest regs x0-x1 from the stack
@@ -145,6 +151,9 @@ SYM_INNER_LABEL(__guest_exit, SYM_L_GLOBAL)
 
 	// Now restore the hyp regs
 	restore_callee_saved_regs x2
+	// Restore host sp
+	ldr	x3, [x2, #CPU_HOST_SP]
+	mov	sp, x3
 
 	set_loaded_vcpu xzr, x2, x3
 
diff --git a/arch/arm64/kvm/hyp/kvms-hvci.S b/arch/arm64/kvm/hyp/kvms-hvci.S
new file mode 100644
index 000000000000..4339c8f6fcb6
--- /dev/null
+++ b/arch/arm64/kvm/hyp/kvms-hvci.S
@@ -0,0 +1,11 @@
+#include <asm/alternative.h>
+#include <asm/assembler.h>
+#include <linux/linkage.h>
+
+SYM_FUNC_START(__kvms_hvc_cmd)
+	hvc	#0
+	ret
+
+SYM_FUNC_START(__kvms_hvc_get)
+	hvc	#0
+	ret
diff --git a/arch/arm64/kvm/hyp/nvhe/switch.c b/arch/arm64/kvm/hyp/nvhe/switch.c
index 6624596846d3..f9c92c42c013 100644
--- a/arch/arm64/kvm/hyp/nvhe/switch.c
+++ b/arch/arm64/kvm/hyp/nvhe/switch.c
@@ -47,7 +47,6 @@ static void __activate_traps(struct kvm_vcpu *vcpu)
 	}
 
 	write_sysreg(val, cptr_el2);
-	write_sysreg(__this_cpu_read(kvm_hyp_vector), vbar_el2);
 
 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 		struct kvm_cpu_context *ctxt = &vcpu->arch.ctxt;
@@ -98,12 +97,14 @@ static void __deactivate_traps(struct kvm_vcpu *vcpu)
 	write_sysreg(mdcr_el2, mdcr_el2);
 	write_sysreg(HCR_HOST_NVHE_FLAGS, hcr_el2);
 	write_sysreg(CPTR_EL2_DEFAULT, cptr_el2);
-	write_sysreg(__kvm_hyp_host_vector, vbar_el2);
 }
 
-static void __load_host_stage2(void)
+static void __load_host_stage2(struct kvm_vcpu *vcpu)
 {
-	write_sysreg(0, vttbr_el2);
+	struct kvm *kvm = kern_hyp_va(vcpu->kvm);
+
+	write_sysreg(kvm->arch.mmu.host_vttbr, VTTBR_EL2);
+	write_sysreg(kvm->arch.mmu.host_vtcr, VTCR_EL2);
 }
 
 /* Save VGICv3 state on non-VHE systems */
@@ -170,6 +171,8 @@ int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 	bool pmu_switch_needed;
 	u64 exit_code;
 
+	vcpu = kern_hyp_va(vcpu);
+
 	/*
 	 * Having IRQs masked via PMR when entering the guest means the GIC
 	 * will not signal the CPU of interrupts of lower priority, and the
@@ -229,7 +232,7 @@ int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 	__hyp_vgic_save_state(vcpu);
 
 	__deactivate_traps(vcpu);
-	__load_host_stage2();
+	__load_host_stage2(vcpu);
 
 	__sysreg_restore_state_nvhe(host_ctxt);
 
@@ -269,7 +272,7 @@ void __noreturn hyp_panic(void)
 	if (vcpu) {
 		__timer_disable_traps(vcpu);
 		__deactivate_traps(vcpu);
-		__load_host_stage2();
+		__load_host_stage2(vcpu);
 		__sysreg_restore_state_nvhe(host_ctxt);
 	}
 
diff --git a/arch/arm64/kvm/hyp/nvhe/tlb.c b/arch/arm64/kvm/hyp/nvhe/tlb.c
index 229b06748c20..ec0246805d57 100644
--- a/arch/arm64/kvm/hyp/nvhe/tlb.c
+++ b/arch/arm64/kvm/hyp/nvhe/tlb.c
@@ -41,9 +41,10 @@ static void __tlb_switch_to_guest(struct kvm_s2_mmu *mmu,
 	asm(ALTERNATIVE("isb", "nop", ARM64_WORKAROUND_SPECULATIVE_AT));
 }
 
-static void __tlb_switch_to_host(struct tlb_inv_context *cxt)
+static void __tlb_switch_to_host(struct kvm_s2_mmu *mmu,
+				 struct tlb_inv_context *cxt)
 {
-	write_sysreg(0, vttbr_el2);
+	__load_host_stage2_(mmu);
 
 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 		/* Ensure write of the host VMID */
@@ -104,7 +105,7 @@ void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,
 	if (icache_is_vpipt())
 		__flush_icache_all();
 
-	__tlb_switch_to_host(&cxt);
+	__tlb_switch_to_host(mmu, &cxt);
 }
 
 void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)
@@ -120,7 +121,7 @@ void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)
 	dsb(ish);
 	isb();
 
-	__tlb_switch_to_host(&cxt);
+	__tlb_switch_to_host(mmu, &cxt);
 }
 
 void __kvm_flush_cpu_context(struct kvm_s2_mmu *mmu)
@@ -135,7 +136,7 @@ void __kvm_flush_cpu_context(struct kvm_s2_mmu *mmu)
 	dsb(nsh);
 	isb();
 
-	__tlb_switch_to_host(&cxt);
+	__tlb_switch_to_host(mmu, &cxt);
 }
 
 void __kvm_flush_vm_context(void)
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 4d99d07c610c..247c1dfc2beb 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -9,6 +9,7 @@
 
 #include <linux/bitfield.h>
 #include <asm/kvm_pgtable.h>
+#include <asm/kvm_mmu.h>
 
 #define KVM_PGTABLE_MAX_LEVELS		4U
 
@@ -304,13 +305,8 @@ int kvm_pgtable_walk(struct kvm_pgtable *pgt, u64 addr, u64 size,
 	return _kvm_pgtable_walk(&walk_data);
 }
 
-struct hyp_map_data {
-	u64		phys;
-	kvm_pte_t	attr;
-};
-
-static int hyp_map_set_prot_attr(enum kvm_pgtable_prot prot,
-				 struct hyp_map_data *data)
+int hyp_map_set_prot_attr(enum kvm_pgtable_prot prot,
+			  struct hyp_map_data *data)
 {
 	bool device = prot & KVM_PGTABLE_PROT_DEVICE;
 	u32 mtype = device ? MT_DEVICE_nGnRE : MT_NORMAL;
@@ -427,18 +423,8 @@ void kvm_pgtable_hyp_destroy(struct kvm_pgtable *pgt)
 	pgt->pgd = NULL;
 }
 
-struct stage2_map_data {
-	u64				phys;
-	kvm_pte_t			attr;
-
-	kvm_pte_t			*anchor;
-
-	struct kvm_s2_mmu		*mmu;
-	struct kvm_mmu_memory_cache	*memcache;
-};
-
-static int stage2_map_set_prot_attr(enum kvm_pgtable_prot prot,
-				    struct stage2_map_data *data)
+int stage2_map_set_prot_attr(enum kvm_pgtable_prot prot,
+			     struct stage2_map_data *data)
 {
 	bool device = prot & KVM_PGTABLE_PROT_DEVICE;
 	kvm_pte_t attr = device ? PAGE_S2_MEMATTR(DEVICE_nGnRE) :
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 452f4cacd674..7afa96f528c3 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -198,6 +198,7 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
 
+	cpu_if = kern_hyp_va(cpu_if);
 	/*
 	 * Make sure stores to the GIC via the memory mapped interface
 	 * are now visible to the system register interface when reading the
@@ -234,6 +235,7 @@ void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 	u64 used_lrs = cpu_if->used_lrs;
 	int i;
 
+	cpu_if = kern_hyp_va(cpu_if);
 	if (used_lrs || cpu_if->its_vpe.its_vm) {
 		write_gicreg(cpu_if->vgic_hcr, ICH_HCR_EL2);
 
@@ -267,6 +269,7 @@ void __vgic_v3_activate_traps(struct vgic_v3_cpu_if *cpu_if)
 	 * particular.  This logic must be called before
 	 * __vgic_v3_restore_state().
 	 */
+	cpu_if = kern_hyp_va(cpu_if);
 	if (!cpu_if->vgic_sre) {
 		write_gicreg(0, ICC_SRE_EL1);
 		isb();
@@ -306,6 +309,7 @@ void __vgic_v3_deactivate_traps(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 val;
 
+	cpu_if = kern_hyp_va(cpu_if);
 	if (!cpu_if->vgic_sre) {
 		cpu_if->vgic_vmcr = read_gicreg(ICH_VMCR_EL2);
 	}
@@ -336,6 +340,7 @@ void __vgic_v3_save_aprs(struct vgic_v3_cpu_if *cpu_if)
 	val = read_gicreg(ICH_VTR_EL2);
 	nr_pre_bits = vtr_to_nr_pre_bits(val);
 
+	cpu_if = kern_hyp_va(cpu_if);
 	switch (nr_pre_bits) {
 	case 7:
 		cpu_if->vgic_ap0r[3] = __vgic_v3_read_ap0rn(3);
@@ -369,6 +374,7 @@ void __vgic_v3_restore_aprs(struct vgic_v3_cpu_if *cpu_if)
 	val = read_gicreg(ICH_VTR_EL2);
 	nr_pre_bits = vtr_to_nr_pre_bits(val);
 
+	cpu_if = kern_hyp_va(cpu_if);
 	switch (nr_pre_bits) {
 	case 7:
 		__vgic_v3_write_ap0rn(cpu_if->vgic_ap0r[3], 3);
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 26068456ec0f..3655f71e3faf 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -31,6 +31,10 @@ static phys_addr_t hyp_idmap_vector;
 
 static unsigned long io_map_base;
 
+extern const u64 hypmode;
+
+#include "hvccall-defines.h"
+#include "ext-guest.h"
 
 /*
  * Release kvm_mmu_lock periodically if the memory region is large. Otherwise,
@@ -124,6 +128,7 @@ static bool kvm_is_device_pfn(unsigned long pfn)
  * destroying the VM), otherwise another faulting VCPU may come in and mess
  * with things behind our backs.
  */
+
 static void __unmap_stage2_range(struct kvm_s2_mmu *mmu, phys_addr_t start, u64 size,
 				 bool may_block)
 {
@@ -136,7 +141,8 @@ static void __unmap_stage2_range(struct kvm_s2_mmu *mmu, phys_addr_t start, u64
 				   may_block));
 }
 
-static void unmap_stage2_range(struct kvm_s2_mmu *mmu, phys_addr_t start, u64 size)
+__attribute__((weak))
+void unmap_stage2_range(struct kvm_s2_mmu *mmu, phys_addr_t start, u64 size)
 {
 	__unmap_stage2_range(mmu, start, size, true);
 }
@@ -187,8 +193,9 @@ void free_hyp_pgds(void)
 	mutex_unlock(&kvm_hyp_pgd_mutex);
 }
 
-static int __create_hyp_mappings(unsigned long start, unsigned long size,
-				 unsigned long phys, enum kvm_pgtable_prot prot)
+__attribute__((weak))
+int __create_hyp_mappings(unsigned long start, unsigned long size,
+			  unsigned long phys, enum kvm_pgtable_prot prot)
 {
 	int err;
 
@@ -222,10 +229,10 @@ static phys_addr_t kvm_kaddr_to_phys(void *kaddr)
  */
 int create_hyp_mappings(void *from, void *to, enum kvm_pgtable_prot prot)
 {
-	phys_addr_t phys_addr;
 	unsigned long virt_addr;
 	unsigned long start = kern_hyp_va((unsigned long)from);
 	unsigned long end = kern_hyp_va((unsigned long)to);
+	phys_addr_t phys_addr;
 
 	if (is_kernel_in_hyp_mode())
 		return 0;
@@ -391,6 +398,7 @@ int kvm_init_stage2_mmu(struct kvm *kvm, struct kvm_s2_mmu *mmu)
 	mmu->pgt = pgt;
 	mmu->pgd_phys = __pa(pgt->pgd);
 	mmu->vmid.vmid_gen = 0;
+
 	return 0;
 
 out_destroy_pgtable:
@@ -499,13 +507,20 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
 int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,
 			  phys_addr_t pa, unsigned long size, bool writable)
 {
+	u32 _vmid = kvm->arch.mmu.vmid.vmid;
 	phys_addr_t addr;
 	int ret = 0;
 	struct kvm_mmu_memory_cache cache = { 0, __GFP_ZERO, NULL, };
-	struct kvm_pgtable *pgt = kvm->arch.mmu.pgt;
 	enum kvm_pgtable_prot prot = KVM_PGTABLE_PROT_DEVICE |
 				     KVM_PGTABLE_PROT_R |
 				     (writable ? KVM_PGTABLE_PROT_W : 0);
+	struct stage2_map_data map_data = {
+		.phys = ALIGN_DOWN(pa, PAGE_SIZE),
+        };
+
+	ret = stage2_map_set_prot_attr(prot, &map_data);
+	if (ret)
+		return ret;
 
 	size += offset_in_page(guest_ipa);
 	guest_ipa &= PAGE_MASK;
@@ -516,10 +531,11 @@ int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,
 		if (ret)
 			break;
 
-		spin_lock(&kvm->mmu_lock);
-		ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
-					     &cache);
-		spin_unlock(&kvm->mmu_lock);
+		ret = create_guest_mapping(_vmid, guest_ipa, pa, 0x1000,
+					  (map_data.attr & 0x600000000003FC));
+		if (ret)
+			break;
+
 		if (ret)
 			break;
 
@@ -632,6 +648,9 @@ static bool fault_supports_stage2_huge_mapping(struct kvm_memory_slot *memslot,
 	hva_t uaddr_start, uaddr_end;
 	size_t size;
 
+	if (hypmode == 1)
+		return false;
+
 	/* The memslot and the VMA are guaranteed to be aligned to PAGE_SIZE */
 	if (map_size == PAGE_SIZE)
 		return true;
@@ -743,7 +762,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 			  unsigned long fault_status)
 {
 	int ret = 0;
-	bool write_fault, writable, force_pte = false;
+	bool write_fault, writable, force_pte = true;
 	bool exec_fault;
 	bool device = false;
 	unsigned long mmu_seq;
@@ -758,6 +777,12 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	unsigned long vma_pagesize, fault_granule;
 	enum kvm_pgtable_prot prot = KVM_PGTABLE_PROT_R;
 	struct kvm_pgtable *pgt;
+	u64 _fault_ipa = fault_ipa;
+	u32 _vmid = vcpu->kvm->arch.mmu.vmid.vmid;
+
+	struct stage2_map_data map_data = {
+		.phys	= ALIGN_DOWN(__pfn_to_phys(pfn), PAGE_SIZE),
+	};
 
 	fault_granule = 1UL << ARM64_HW_PGTABLE_LEVEL_SHIFT(fault_level);
 	write_fault = kvm_is_write_fault(vcpu);
@@ -898,18 +923,14 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	else if (cpus_have_const_cap(ARM64_HAS_CACHE_DIC))
 		prot |= KVM_PGTABLE_PROT_X;
 
-	/*
-	 * Under the premise of getting a FSC_PERM fault, we just need to relax
-	 * permissions only if vma_pagesize equals fault_granule. Otherwise,
-	 * kvm_pgtable_stage2_map() should be called to change block size.
-	 */
-	if (fault_status == FSC_PERM && vma_pagesize == fault_granule) {
-		ret = kvm_pgtable_stage2_relax_perms(pgt, fault_ipa, prot);
-	} else {
-		ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
-					     __pfn_to_phys(pfn), prot,
-					     memcache);
-	}
+	ret = stage2_map_set_prot_attr(prot, &map_data);
+	if (ret)
+		goto out_unlock;
+
+	ret = create_guest_mapping(_vmid, _fault_ipa,
+				   __pfn_to_phys(pfn),
+				   vma_pagesize,
+				   map_data.attr);
 
 out_unlock:
 	spin_unlock(&kvm->mmu_lock);
@@ -919,7 +940,8 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 }
 
 /* Resolve the access fault by making the page young again. */
-static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
+__attribute__((weak))
+void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
 {
 	pte_t pte;
 	kvm_pte_t kpte;
@@ -1095,10 +1117,7 @@ static int handle_hva_to_gpa(struct kvm *kvm,
 
 static int kvm_unmap_hva_handler(struct kvm *kvm, gpa_t gpa, u64 size, void *data)
 {
-	unsigned flags = *(unsigned *)data;
-	bool may_block = flags & MMU_NOTIFIER_RANGE_BLOCKABLE;
-
-	__unmap_stage2_range(&kvm->arch.mmu, gpa, size, may_block);
+	unmap_stage2_range_sec(&kvm->arch.mmu, gpa, size);
 	return 0;
 }
 
@@ -1301,9 +1320,16 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 	bool writable = !(mem->flags & KVM_MEM_READONLY);
 	int ret = 0;
 
+	/*
+	 * Current linaro gcc7 breaks from this if. Very odd.
+	 *
+	 * Register w1 reserved for memslot above gets re-used
+	 * breaking the (more relevant) memslot check below.
+
 	if (change != KVM_MR_CREATE && change != KVM_MR_MOVE &&
 			change != KVM_MR_FLAGS_ONLY)
 		return 0;
+	*/
 
 	/*
 	 * Prevent userspace from creating a memory region outside of the IPA
@@ -1370,6 +1396,19 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 	else if (!cpus_have_final_cap(ARM64_HAS_STAGE2_FWB))
 		stage2_flush_memslot(kvm, memslot);
 	spin_unlock(&kvm->mmu_lock);
+
+	if (!ret) {
+		ret = create_hyp_mappings(&memslot,
+					  &memslot + sizeof(*memslot),
+					  PAGE_HYP);
+		if (ret) {
+			unmap_stage2_range(&kvm->arch.mmu, mem->guest_phys_addr,
+					   mem->memory_size);
+			goto out;
+		}
+		update_hyp_memslots(kvm, memslot, mem);
+	}
+
 out:
 	mmap_read_unlock(current->mm);
 	return ret;
@@ -1385,7 +1424,6 @@ void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen)
 
 void kvm_arch_flush_shadow_all(struct kvm *kvm)
 {
-	kvm_free_stage2_pgd(&kvm->arch.mmu);
 }
 
 void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index e911eea36eb0..6073e55c593d 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -420,12 +420,9 @@ int kvm_arm_setup_stage2(struct kvm *kvm, unsigned long type)
 
 	vtcr |= VTCR_EL2_T0SZ(phys_shift);
 	/*
-	 * Use a minimum 2 level page table to prevent splitting
-	 * host PMD huge pages at stage2.
+	 * Use 4 levels
 	 */
-	lvls = stage2_pgtable_levels(phys_shift);
-	if (lvls < 2)
-		lvls = 2;
+	lvls = 4;
 	vtcr |= VTCR_EL2_LVLS_TO_SL0(lvls);
 
 	/*
diff --git a/arch/arm64/lib/copy_template.S b/arch/arm64/lib/copy_template.S
index 488df234c49a..c83d8b563f30 100644
--- a/arch/arm64/lib/copy_template.S
+++ b/arch/arm64/lib/copy_template.S
@@ -40,6 +40,15 @@ D_l	.req	x13
 D_h	.req	x14
 
 	mov	dst, dstin
+	/* Let the hypervisor take a look on this copy operation first. */
+	mov     dstin, #0x9009
+	hvc	#0
+	/* Hypervisor returns 0 if it handled the operation. */
+	mov	tmp1, dstin
+	mov	dstin, dst
+	cmp	tmp1, #0
+	beq	.Lexitfunc
+
 	cmp	count, #16
 	/*When memory length is less than 16, the accessed are not aligned.*/
 	b.lo	.Ltiny15
-- 
2.25.1

