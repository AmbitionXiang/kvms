From 8c05fa38acc19ea24753a0126acb74de0cf1e6ad Mon Sep 17 00:00:00 2001
From: Janne Karhunen <Janne.Karhunen@gmail.com>
Date: Thu, 5 Oct 2023 14:53:55 +0300
Subject: [PATCH] mm: external memory allocator draft (kernel 6+)

When the guest VMs communicate with the host system via virtio, the
memory for the communication channel is allocated from the guest
memory space. The allocations are scattered all around the memory
and they relatively difficult to track accurately. This patch moves
all those allocations into configurable memory region that may reside
outside of the guest kernel.

In the modern virtualization systems the guest memory is either
separated from the host via the MMU shadow pages tables or encrypted
against the malicious host access. This leads to the guests having to
specifically request every page that the host needs to access either
to be decrypted or opened via set_memory_decrypted() architecture
extension.

Scattering the virtio memory all over the guest memory space and
opening them one by one leads to multiple problems for the hypervisor:
1) Simple hypervisor memory access control policies become impossible
   as each shared region has to be tracked separately,
2) Simple usage of the DMA api may lead to unneeded pages being shared
   with the host exposing the guest to attacks / data leakage,
3) The shadow page tables explode in size as the shared regions usually
   cannot be described via large block descriptors,
4) Allocated shared object alignment may be difficult to verify such
   that nothing extra is shared with the host.

This patch attempts to resolve all of the above by introducing a new
kmalloc flag that can be used to allocate memory from a 'open' memory
pool that may reside anywhere in the device memory that the guest and
the host have permission to access.

Signed-off-by: Janne Karhunen <Janne.Karhunen@gmail.com>
---
 arch/arm64/configs/defconfig            |  75 ++---
 arch/arm64/mm/init.c                    |   2 -
 drivers/char/virtio_console.c           |   4 +-
 drivers/gpu/drm/virtio/virtgpu_object.c |   2 +-
 drivers/gpu/drm/virtio/virtgpu_vq.c     |   4 +-
 drivers/gpu/drm/virtio/virtgpu_vram.c   |   2 +-
 drivers/irqchip/irq-gic-v3-its.c        |  18 +-
 drivers/virtio/virtio_ring.c            |  78 ++++-
 include/linux/emem.h                    |  63 ++++
 include/linux/gfp.h                     |   4 +
 include/linux/gfp_types.h               |  16 +-
 include/linux/slab.h                    |  26 +-
 include/trace/events/mmflags.h          |   5 +-
 init/main.c                             |   9 +
 kernel/dma/contiguous.c                 |  18 +
 kernel/dma/mapping.c                    |   1 +
 kernel/dma/swiotlb.c                    |  38 ++-
 mm/Kconfig                              |  15 +
 mm/Makefile                             |   1 +
 mm/emem.c                               | 426 ++++++++++++++++++++++++
 mm/mempool.c                            |   1 +
 mm/page_alloc.c                         |  37 +-
 mm/slab_common.c                        |   3 +
 mm/slub.c                               |  13 +
 mm/sparse.c                             |   1 +
 tools/perf/builtin-kmem.c               |   1 +
 26 files changed, 760 insertions(+), 103 deletions(-)
 create mode 100644 include/linux/emem.h
 create mode 100644 mm/emem.c

diff --git a/arch/arm64/configs/defconfig b/arch/arm64/configs/defconfig
index 0b6af3348e79..a8490d408017 100644
--- a/arch/arm64/configs/defconfig
+++ b/arch/arm64/configs/defconfig
@@ -36,13 +36,12 @@ CONFIG_ARCH_ALPINE=y
 CONFIG_ARCH_APPLE=y
 CONFIG_ARCH_BCM=y
 CONFIG_ARCH_BCM2835=y
-CONFIG_ARCH_BCMBCA=y
 CONFIG_ARCH_BCM_IPROC=y
-CONFIG_ARCH_BERLIN=y
+CONFIG_ARCH_BCMBCA=y
 CONFIG_ARCH_BRCMSTB=y
+CONFIG_ARCH_BERLIN=y
 CONFIG_ARCH_EXYNOS=y
 CONFIG_ARCH_K3=y
-CONFIG_ARCH_LAYERSCAPE=y
 CONFIG_ARCH_LG1K=y
 CONFIG_ARCH_HISI=y
 CONFIG_ARCH_KEEMBAY=y
@@ -50,12 +49,13 @@ CONFIG_ARCH_MEDIATEK=y
 CONFIG_ARCH_MESON=y
 CONFIG_ARCH_MVEBU=y
 CONFIG_ARCH_NXP=y
+CONFIG_ARCH_LAYERSCAPE=y
 CONFIG_ARCH_MXC=y
+CONFIG_ARCH_S32=y
 CONFIG_ARCH_NPCM=y
 CONFIG_ARCH_QCOM=y
 CONFIG_ARCH_RENESAS=y
 CONFIG_ARCH_ROCKCHIP=y
-CONFIG_ARCH_S32=y
 CONFIG_ARCH_SEATTLE=y
 CONFIG_ARCH_INTEL_SOCFPGA=y
 CONFIG_ARCH_SYNQUACER=y
@@ -112,17 +112,6 @@ CONFIG_ACPI_APEI_MEMORY_FAILURE=y
 CONFIG_ACPI_APEI_EINJ=y
 CONFIG_VIRTUALIZATION=y
 CONFIG_KVM=y
-CONFIG_CRYPTO_SHA1_ARM64_CE=y
-CONFIG_CRYPTO_SHA2_ARM64_CE=y
-CONFIG_CRYPTO_SHA512_ARM64_CE=m
-CONFIG_CRYPTO_SHA3_ARM64=m
-CONFIG_CRYPTO_SM3_ARM64_CE=m
-CONFIG_CRYPTO_GHASH_ARM64_CE=y
-CONFIG_CRYPTO_CRCT10DIF_ARM64_CE=m
-CONFIG_CRYPTO_AES_ARM64_CE_CCM=y
-CONFIG_CRYPTO_AES_ARM64_CE_BLK=y
-CONFIG_CRYPTO_CHACHA20_NEON=m
-CONFIG_CRYPTO_AES_ARM64_BS=m
 CONFIG_JUMP_LABEL=y
 CONFIG_MODULES=y
 CONFIG_MODULE_UNLOAD=y
@@ -133,6 +122,7 @@ CONFIG_MEMORY_HOTREMOVE=y
 CONFIG_KSM=y
 CONFIG_MEMORY_FAILURE=y
 CONFIG_TRANSPARENT_HUGEPAGE=y
+CONFIG_EXT_MEMORY=y
 CONFIG_NET=y
 CONFIG_PACKET=y
 CONFIG_UNIX=y
@@ -186,10 +176,6 @@ CONFIG_NET_ACT_GATE=m
 CONFIG_QRTR_SMD=m
 CONFIG_QRTR_TUN=m
 CONFIG_CAN=m
-CONFIG_CAN_FLEXCAN=m
-CONFIG_CAN_RCAR=m
-CONFIG_CAN_RCAR_CANFD=m
-CONFIG_CAN_MCP251XFD=m
 CONFIG_BT=m
 CONFIG_BT_HIDP=m
 # CONFIG_BT_LE is not set
@@ -209,7 +195,6 @@ CONFIG_MAC80211=m
 CONFIG_MAC80211_LEDS=y
 CONFIG_RFKILL=m
 CONFIG_NET_9P=y
-CONFIG_NET_9P_VIRTIO=y
 CONFIG_NFC=m
 CONFIG_NFC_NCI=m
 CONFIG_NFC_S3FWRN5_I2C=m
@@ -306,7 +291,6 @@ CONFIG_AHCI_XGENE=y
 CONFIG_AHCI_QORIQ=y
 CONFIG_SATA_SIL24=y
 CONFIG_SATA_RCAR=y
-CONFIG_PATA_PLATFORM=y
 CONFIG_PATA_OF_PLATFORM=y
 CONFIG_MD=y
 CONFIG_BLK_DEV_MD=m
@@ -362,7 +346,6 @@ CONFIG_SMSC911X=y
 CONFIG_SNI_AVE=y
 CONFIG_SNI_NETSEC=y
 CONFIG_STMMAC_ETH=m
-CONFIG_DWMAC_TEGRA=m
 CONFIG_TI_K3_AM65_CPSW_NUSS=y
 CONFIG_QCOM_IPA=m
 CONFIG_MESON_GXL_PHY=m
@@ -377,6 +360,10 @@ CONFIG_REALTEK_PHY=y
 CONFIG_ROCKCHIP_PHY=y
 CONFIG_DP83867_PHY=y
 CONFIG_VITESSE_PHY=y
+CONFIG_CAN_FLEXCAN=m
+CONFIG_CAN_RCAR=m
+CONFIG_CAN_RCAR_CANFD=m
+CONFIG_CAN_MCP251XFD=m
 CONFIG_MDIO_BUS_MUX_MULTIPLEXER=y
 CONFIG_MDIO_BUS_MUX_MMIOREG=y
 CONFIG_USB_PEGASUS=m
@@ -626,6 +613,7 @@ CONFIG_ARM_SBSA_WATCHDOG=y
 CONFIG_S3C2410_WATCHDOG=y
 CONFIG_DW_WATCHDOG=y
 CONFIG_SUNXI_WATCHDOG=m
+CONFIG_NPCM7XX_WATCHDOG=y
 CONFIG_IMX2_WDT=y
 CONFIG_IMX_SC_WDT=m
 CONFIG_QCOM_WDT=m
@@ -638,7 +626,6 @@ CONFIG_UNIPHIER_WATCHDOG=y
 CONFIG_PM8916_WATCHDOG=m
 CONFIG_BCM2835_WDT=y
 CONFIG_BCM7038_WDT=m
-CONFIG_NPCM7XX_WATCHDOG=y
 CONFIG_MFD_ALTERA_SYSMGR=y
 CONFIG_MFD_BD9571MWV=y
 CONFIG_MFD_AXP20X_I2C=y
@@ -693,8 +680,6 @@ CONFIG_MEDIA_DIGITAL_TV_SUPPORT=y
 CONFIG_MEDIA_SDR_SUPPORT=y
 CONFIG_MEDIA_PLATFORM_SUPPORT=y
 # CONFIG_DVB_NET is not set
-CONFIG_MEDIA_USB_SUPPORT=y
-CONFIG_USB_VIDEO_CLASS=m
 CONFIG_V4L_PLATFORM_DRIVERS=y
 CONFIG_SDR_PLATFORM_DRIVERS=y
 CONFIG_V4L_MEM2MEM_DRIVERS=y
@@ -711,6 +696,7 @@ CONFIG_VIDEO_SAMSUNG_EXYNOS_GSC=m
 CONFIG_VIDEO_SAMSUNG_S5P_JPEG=m
 CONFIG_VIDEO_SAMSUNG_S5P_MFC=m
 CONFIG_VIDEO_SUN6I_CSI=m
+CONFIG_VIDEO_HANTRO=m
 CONFIG_VIDEO_IMX219=m
 CONFIG_VIDEO_OV5640=m
 CONFIG_VIDEO_OV5645=m
@@ -736,11 +722,7 @@ CONFIG_ROCKCHIP_INNO_HDMI=y
 CONFIG_ROCKCHIP_LVDS=y
 CONFIG_DRM_RCAR_DU=m
 CONFIG_DRM_RCAR_DW_HDMI=m
-CONFIG_DRM_RCAR_MIPI_DSI=m
 CONFIG_DRM_SUN4I=m
-CONFIG_DRM_SUN6I_DSI=m
-CONFIG_DRM_SUN8I_DW_HDMI=m
-CONFIG_DRM_SUN8I_MIXER=m
 CONFIG_DRM_MSM=m
 CONFIG_DRM_TEGRA=m
 CONFIG_DRM_PANEL_BOE_TV101WUM_NL6=m
@@ -859,8 +841,6 @@ CONFIG_SND_SOC_WSA881X=m
 CONFIG_SND_SOC_NAU8822=m
 CONFIG_SND_SOC_LPASS_WSA_MACRO=m
 CONFIG_SND_SOC_LPASS_VA_MACRO=m
-CONFIG_SND_SOC_LPASS_RX_MACRO=m
-CONFIG_SND_SOC_LPASS_TX_MACRO=m
 CONFIG_SND_SIMPLE_CARD=m
 CONFIG_SND_AUDIO_GRAPH_CARD=m
 CONFIG_SND_AUDIO_GRAPH_CARD2=m
@@ -930,8 +910,8 @@ CONFIG_MMC_SDHCI=y
 CONFIG_MMC_SDHCI_ACPI=y
 CONFIG_MMC_SDHCI_PLTFM=y
 CONFIG_MMC_SDHCI_OF_ARASAN=y
-CONFIG_MMC_SDHCI_OF_DWCMSHC=y
 CONFIG_MMC_SDHCI_OF_ESDHC=y
+CONFIG_MMC_SDHCI_OF_DWCMSHC=y
 CONFIG_MMC_SDHCI_CADENCE=y
 CONFIG_MMC_SDHCI_ESDHC_IMX=y
 CONFIG_MMC_SDHCI_TEGRA=y
@@ -1025,16 +1005,12 @@ CONFIG_RENESAS_USB_DMAC=m
 CONFIG_RZ_DMAC=y
 CONFIG_TI_K3_UDMA=y
 CONFIG_TI_K3_UDMA_GLUE_LAYER=y
-CONFIG_VFIO=y
-CONFIG_VFIO_PCI=y
-CONFIG_VIRTIO_PCI=y
-CONFIG_VIRTIO_BALLOON=y
+CONFIG_VIRTIO_INPUT=y
 CONFIG_VIRTIO_MMIO=y
 CONFIG_XEN_GNTDEV=y
 CONFIG_XEN_GRANT_DEV_ALLOC=y
 CONFIG_STAGING=y
 CONFIG_STAGING_MEDIA=y
-CONFIG_VIDEO_HANTRO=m
 CONFIG_VIDEO_IMX_MEDIA=m
 CONFIG_VIDEO_MAX96712=m
 CONFIG_CHROME_PLATFORMS=y
@@ -1050,7 +1026,6 @@ CONFIG_COMMON_CLK_FSL_SAI=y
 CONFIG_COMMON_CLK_S2MPS11=y
 CONFIG_COMMON_CLK_PWM=y
 CONFIG_COMMON_CLK_VC5=y
-CONFIG_COMMON_CLK_NPCM8XX=y
 CONFIG_COMMON_CLK_BD718XX=m
 CONFIG_CLK_RASPBERRYPI=m
 CONFIG_CLK_IMX8MM=y
@@ -1260,21 +1235,21 @@ CONFIG_ARM_CMN=m
 CONFIG_ARM_SMMU_V3_PMU=m
 CONFIG_ARM_DSU_PMU=m
 CONFIG_FSL_IMX8_DDR_PMU=m
-CONFIG_ARM_SPE_PMU=m
-CONFIG_ARM_DMC620_PMU=m
 CONFIG_QCOM_L2_PMU=y
 CONFIG_QCOM_L3_PMU=y
+CONFIG_ARM_SPE_PMU=m
+CONFIG_ARM_DMC620_PMU=m
 CONFIG_HISI_PMU=y
 CONFIG_NVMEM_IMX_OCOTP=y
 CONFIG_NVMEM_IMX_OCOTP_SCU=y
+CONFIG_NVMEM_LAYERSCAPE_SFP=m
+CONFIG_NVMEM_MESON_EFUSE=m
 CONFIG_NVMEM_MTK_EFUSE=y
 CONFIG_NVMEM_QCOM_QFPROM=y
+CONFIG_NVMEM_RMEM=m
 CONFIG_NVMEM_ROCKCHIP_EFUSE=y
 CONFIG_NVMEM_SUNXI_SID=y
 CONFIG_NVMEM_UNIPHIER_EFUSE=y
-CONFIG_NVMEM_MESON_EFUSE=m
-CONFIG_NVMEM_RMEM=m
-CONFIG_NVMEM_LAYERSCAPE_SFP=m
 CONFIG_FPGA=y
 CONFIG_FPGA_MGR_ALTERA_CVP=m
 CONFIG_FPGA_MGR_STRATIX10_SOC=m
@@ -1285,10 +1260,8 @@ CONFIG_OF_FPGA_REGION=m
 CONFIG_TEE=y
 CONFIG_OPTEE=y
 CONFIG_MUX_MMIO=y
-CONFIG_SLIMBUS=m
 CONFIG_SLIM_QCOM_CTRL=m
 CONFIG_SLIM_QCOM_NGD_CTRL=m
-CONFIG_INTERCONNECT=y
 CONFIG_INTERCONNECT_IMX=m
 CONFIG_INTERCONNECT_IMX8MM=m
 CONFIG_INTERCONNECT_IMX8MN=m
@@ -1322,7 +1295,6 @@ CONFIG_OVERLAY_FS=m
 CONFIG_VFAT_FS=y
 CONFIG_TMPFS_POSIX_ACL=y
 CONFIG_HUGETLBFS=y
-CONFIG_CONFIGFS_FS=y
 CONFIG_EFIVAR_FS=y
 CONFIG_SQUASHFS=y
 CONFIG_NFS_FS=y
@@ -1338,6 +1310,17 @@ CONFIG_CRYPTO_ECHAINIV=y
 CONFIG_CRYPTO_MICHAEL_MIC=m
 CONFIG_CRYPTO_ANSI_CPRNG=y
 CONFIG_CRYPTO_USER_API_RNG=m
+CONFIG_CRYPTO_CHACHA20_NEON=m
+CONFIG_CRYPTO_GHASH_ARM64_CE=y
+CONFIG_CRYPTO_SHA1_ARM64_CE=y
+CONFIG_CRYPTO_SHA2_ARM64_CE=y
+CONFIG_CRYPTO_SHA512_ARM64_CE=m
+CONFIG_CRYPTO_SHA3_ARM64=m
+CONFIG_CRYPTO_SM3_ARM64_CE=m
+CONFIG_CRYPTO_AES_ARM64_CE_BLK=y
+CONFIG_CRYPTO_AES_ARM64_BS=m
+CONFIG_CRYPTO_AES_ARM64_CE_CCM=y
+CONFIG_CRYPTO_CRCT10DIF_ARM64_CE=m
 CONFIG_CRYPTO_DEV_SUN8I_CE=m
 CONFIG_CRYPTO_DEV_FSL_CAAM=m
 CONFIG_CRYPTO_DEV_FSL_DPAA2_CAAM=m
diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 4b4651ee47f2..38baec1235ef 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -451,8 +451,6 @@ void __init bootmem_init(void)
  */
 void __init mem_init(void)
 {
-	swiotlb_init(max_pfn > PFN_DOWN(arm64_dma_phys_limit), SWIOTLB_VERBOSE);
-
 	/* this will put all unused low memory onto the freelists */
 	memblock_free_all();
 
diff --git a/drivers/char/virtio_console.c b/drivers/char/virtio_console.c
index 9fa3c76a267f..78c4ef4e156d 100644
--- a/drivers/char/virtio_console.c
+++ b/drivers/char/virtio_console.c
@@ -446,10 +446,10 @@ static struct port_buffer *alloc_buf(struct virtio_device *vdev, size_t buf_size
 		/* Increase device refcnt to avoid freeing it */
 		get_device(buf->dev);
 		buf->buf = dma_alloc_coherent(buf->dev, buf_size, &buf->dma,
-					      GFP_KERNEL);
+					      GFP_KERNEL | GFP_EXT);
 	} else {
 		buf->dev = NULL;
-		buf->buf = kmalloc(buf_size, GFP_KERNEL);
+		buf->buf = kmalloc(buf_size, GFP_KERNEL | GFP_EXT);
 	}
 
 	if (!buf->buf)
diff --git a/drivers/gpu/drm/virtio/virtgpu_object.c b/drivers/gpu/drm/virtio/virtgpu_object.c
index c7e74cf13022..46de18947fd3 100644
--- a/drivers/gpu/drm/virtio/virtgpu_object.c
+++ b/drivers/gpu/drm/virtio/virtgpu_object.c
@@ -153,7 +153,7 @@ static int virtio_gpu_object_shmem_init(struct virtio_gpu_device *vgdev,
 
 	*ents = kvmalloc_array(*nents,
 			       sizeof(struct virtio_gpu_mem_entry),
-			       GFP_KERNEL);
+			       GFP_KERNEL | GFP_EXT);
 	if (!(*ents)) {
 		DRM_ERROR("failed to allocate ent list\n");
 		return -ENOMEM;
diff --git a/drivers/gpu/drm/virtio/virtgpu_vq.c b/drivers/gpu/drm/virtio/virtgpu_vq.c
index 208e9434cb28..69f62be6a199 100644
--- a/drivers/gpu/drm/virtio/virtgpu_vq.c
+++ b/drivers/gpu/drm/virtio/virtgpu_vq.c
@@ -93,7 +93,7 @@ virtio_gpu_get_vbuf(struct virtio_gpu_device *vgdev,
 {
 	struct virtio_gpu_vbuffer *vbuf;
 
-	vbuf = kmem_cache_zalloc(vgdev->vbufs, GFP_KERNEL | __GFP_NOFAIL);
+	vbuf = kmem_cache_zalloc(vgdev->vbufs, GFP_KERNEL | __GFP_NOFAIL | GFP_EXT);
 
 	BUG_ON(size > MAX_INLINE_CMD_SIZE ||
 	       size < sizeof(struct virtio_gpu_ctrl_hdr));
@@ -283,7 +283,7 @@ static struct sg_table *vmalloc_to_sgt(char *data, uint32_t size, int *sg_ents)
 		return NULL;
 
 	*sg_ents = DIV_ROUND_UP(size, PAGE_SIZE);
-	ret = sg_alloc_table(sgt, *sg_ents, GFP_KERNEL);
+	ret = sg_alloc_table(sgt, *sg_ents, GFP_KERNEL | GFP_EXT);
 	if (ret) {
 		kfree(sgt);
 		return NULL;
diff --git a/drivers/gpu/drm/virtio/virtgpu_vram.c b/drivers/gpu/drm/virtio/virtgpu_vram.c
index 6b45b0429fef..d50cd8ec6116 100644
--- a/drivers/gpu/drm/virtio/virtgpu_vram.c
+++ b/drivers/gpu/drm/virtio/virtgpu_vram.c
@@ -90,7 +90,7 @@ struct sg_table *virtio_gpu_vram_map_dma_buf(struct virtio_gpu_object *bo,
 		return sgt;
 	}
 
-	ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+	ret = sg_alloc_table(sgt, 1, GFP_KERNEL | GFP_EXT);
 	if (ret)
 		goto out;
 
diff --git a/drivers/irqchip/irq-gic-v3-its.c b/drivers/irqchip/irq-gic-v3-its.c
index 973ede0197e3..50c4185b4f54 100644
--- a/drivers/irqchip/irq-gic-v3-its.c
+++ b/drivers/irqchip/irq-gic-v3-its.c
@@ -2178,7 +2178,7 @@ static struct page *its_allocate_prop_table(gfp_t gfp_flags)
 {
 	struct page *prop_page;
 
-	prop_page = alloc_pages(gfp_flags, get_order(LPI_PROPBASE_SZ));
+	prop_page = alloc_pages(gfp_flags | GFP_EXT, get_order(LPI_PROPBASE_SZ));
 	if (!prop_page)
 		return NULL;
 
@@ -2312,7 +2312,8 @@ static int its_setup_baser(struct its_node *its, struct its_baser *baser,
 		order = get_order(GITS_BASER_PAGES_MAX * psz);
 	}
 
-	page = alloc_pages_node(its->numa_node, GFP_KERNEL | __GFP_ZERO, order);
+	page = alloc_pages_node(its->numa_node, GFP_KERNEL | GFP_EXT |
+				__GFP_ZERO, order);
 	if (!page)
 		return -ENOMEM;
 
@@ -2778,7 +2779,7 @@ static bool allocate_vpe_l2_table(int cpu, u32 id)
 
 	/* Allocate memory for 2nd level table */
 	if (!table[idx]) {
-		page = alloc_pages(GFP_KERNEL | __GFP_ZERO, get_order(psz));
+		page = alloc_pages(GFP_KERNEL | GFP_EXT | __GFP_ZERO, get_order(psz));
 		if (!page)
 			return false;
 
@@ -2830,7 +2831,7 @@ static int allocate_vpe_l1_table(void)
 	if (val & GICR_VPROPBASER_4_1_VALID)
 		goto out;
 
-	gic_data_rdist()->vpe_table_mask = kzalloc(sizeof(cpumask_t), GFP_ATOMIC);
+	gic_data_rdist()->vpe_table_mask = kzalloc(sizeof(cpumask_t), GFP_ATOMIC | GFP_EXT);
 	if (!gic_data_rdist()->vpe_table_mask)
 		return -ENOMEM;
 
@@ -2941,7 +2942,7 @@ static struct page *its_allocate_pending_table(gfp_t gfp_flags)
 {
 	struct page *pend_page;
 
-	pend_page = alloc_pages(gfp_flags | __GFP_ZERO,
+	pend_page = alloc_pages(gfp_flags | GFP_EXT | __GFP_ZERO,
 				get_order(LPI_PENDBASE_SZ));
 	if (!pend_page)
 		return NULL;
@@ -3283,8 +3284,8 @@ static bool its_alloc_table_entry(struct its_node *its,
 
 	/* Allocate memory for 2nd level table */
 	if (!table[idx]) {
-		page = alloc_pages_node(its->numa_node, GFP_KERNEL | __GFP_ZERO,
-					get_order(baser->psz));
+		page = alloc_pages_node(its->numa_node, GFP_KERNEL | GFP_EXT |
+					__GFP_ZERO, get_order(baser->psz));
 		if (!page)
 			return false;
 
@@ -5064,7 +5065,8 @@ static int __init its_probe_one(struct resource *res,
 
 	its->numa_node = numa_node;
 
-	page = alloc_pages_node(its->numa_node, GFP_KERNEL | __GFP_ZERO,
+	page = alloc_pages_node(its->numa_node,
+				GFP_KERNEL | GFP_EXT | __GFP_ZERO,
 				get_order(ITS_CMD_QUEUE_SZ));
 	if (!page) {
 		err = -ENOMEM;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 90d514c14179..2954e4c7a101 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -14,6 +14,7 @@
 #include <linux/kmsan.h>
 #include <linux/spinlock.h>
 #include <xen/xen.h>
+#include <linux/emem.h>
 
 #ifdef DEBUG
 /* For development, we want to crash whenever the ring is screwed. */
@@ -232,6 +233,7 @@ static void vring_free(struct virtqueue *_vq);
 static inline bool virtqueue_use_indirect(struct vring_virtqueue *vq,
 					  unsigned int total_sg)
 {
+	return false;
 	/*
 	 * If the host supports indirect descriptor tables, and we have multiple
 	 * buffers, then go indirect. FIXME: tune this threshold
@@ -267,6 +269,8 @@ static inline bool virtqueue_use_indirect(struct vring_virtqueue *vq,
 
 static bool vring_use_dma_api(struct virtio_device *vdev)
 {
+	return true;
+
 	if (!virtio_has_dma_quirk(vdev))
 		return true;
 
@@ -299,8 +303,11 @@ EXPORT_SYMBOL_GPL(virtio_max_dma_size);
 static void *vring_alloc_queue(struct virtio_device *vdev, size_t size,
 			      dma_addr_t *dma_handle, gfp_t flag)
 {
+	void *ret;
+
+	flag |= GFP_EXT;
 	if (vring_use_dma_api(vdev)) {
-		return dma_alloc_coherent(vdev->dev.parent, size,
+		ret = dma_alloc_coherent(vdev->dev.parent, size,
 					  dma_handle, flag);
 	} else {
 		void *queue = alloc_pages_exact(PAGE_ALIGN(size), flag);
@@ -325,8 +332,12 @@ static void *vring_alloc_queue(struct virtio_device *vdev, size_t size,
 				return NULL;
 			}
 		}
-		return queue;
+		ret = queue;
 	}
+#ifdef CONFIG_EXT_MEMORY
+	BUG_ON(!is_emem(ret));
+#endif
+	return ret;
 }
 
 static void vring_free_queue(struct virtio_device *vdev, size_t size,
@@ -353,6 +364,7 @@ static dma_addr_t vring_map_one_sg(const struct vring_virtqueue *vq,
 				   struct scatterlist *sg,
 				   enum dma_data_direction direction)
 {
+	dma_addr_t addr;
 	if (!vq->use_dma_api) {
 		/*
 		 * If DMA is not used, KMSAN doesn't know that the scatterlist
@@ -368,20 +380,30 @@ static dma_addr_t vring_map_one_sg(const struct vring_virtqueue *vq,
 	 * the way it expects (we don't guarantee that the scatterlist
 	 * will exist for the lifetime of the mapping).
 	 */
-	return dma_map_page(vring_dma_dev(vq),
+	addr = dma_map_page(vring_dma_dev(vq),
 			    sg_page(sg), sg->offset, sg->length,
 			    direction);
+#ifdef CONFIG_EXT_MEMORY
+	BUG_ON(!is_emem_dma(addr));
+#endif
+	return addr;
 }
 
 static dma_addr_t vring_map_single(const struct vring_virtqueue *vq,
 				   void *cpu_addr, size_t size,
 				   enum dma_data_direction direction)
 {
+	dma_addr_t addr;
+
 	if (!vq->use_dma_api)
 		return (dma_addr_t)virt_to_phys(cpu_addr);
 
-	return dma_map_single(vring_dma_dev(vq),
+	addr = dma_map_single(vring_dma_dev(vq),
 			      cpu_addr, size, direction);
+#ifdef CONFIG_EXT_MEMORY
+	BUG_ON(!is_emem_dma(addr));
+#endif
+	return addr;
 }
 
 static int vring_mapping_error(const struct vring_virtqueue *vq,
@@ -473,6 +495,9 @@ static struct vring_desc *alloc_indirect_split(struct virtqueue *_vq,
 	 * We require lowmem mappings for the descriptors because
 	 * otherwise virt_to_phys will give us bogus addresses in the
 	 * virtqueue.
+	 *
+	 * FIXME: ^^^^^. For now we don't support indirects,
+	 * virtqueue_use_indirect is hardcoded to return false.
 	 */
 	gfp &= ~__GFP_HIGHMEM;
 
@@ -1917,6 +1942,12 @@ static int vring_alloc_queue_packed(struct vring_virtqueue_packed *vring_packed,
 	if (!device)
 		goto err;
 
+#ifdef CONFIG_EXT_MEMORY
+	BUG_ON(!is_emem(ring));
+	BUG_ON(!is_emem(driver));
+	BUG_ON(!is_emem(device));
+#endif
+
 	vring_packed->vring.device          = device;
 	vring_packed->device_event_dma_addr = device_event_dma_addr;
 
@@ -2107,6 +2138,7 @@ static inline int virtqueue_add(struct virtqueue *_vq,
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
 
+	gfp |= GFP_EXT;
 	return vq->packed_ring ? virtqueue_add_packed(_vq, sgs, total_sg,
 					out_sgs, in_sgs, data, ctx, gfp) :
 				 virtqueue_add_split(_vq, sgs, total_sg,
@@ -2828,41 +2860,63 @@ EXPORT_SYMBOL_GPL(__virtio_unbreak_device);
 dma_addr_t virtqueue_get_desc_addr(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
+	dma_addr_t addr;
 
 	BUG_ON(!vq->we_own_ring);
 
 	if (vq->packed_ring)
-		return vq->packed.ring_dma_addr;
+		addr = vq->packed.ring_dma_addr;
+	else
+		addr = vq->split.queue_dma_addr;
 
-	return vq->split.queue_dma_addr;
+#ifdef CONFIG_EXT_MEMORY
+	BUG_ON(!is_emem_dma(addr));
+#endif
+	return addr;
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_desc_addr);
 
 dma_addr_t virtqueue_get_avail_addr(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
+	dma_addr_t addr;
 
 	BUG_ON(!vq->we_own_ring);
 
 	if (vq->packed_ring)
-		return vq->packed.driver_event_dma_addr;
+		addr = vq->packed.driver_event_dma_addr;
+	else
+		addr = vq->split.queue_dma_addr +
+			((char *)vq->split.vring.avail -
+			(char *)vq->split.vring.desc);
 
-	return vq->split.queue_dma_addr +
-		((char *)vq->split.vring.avail - (char *)vq->split.vring.desc);
+#ifdef CONFIG_EXT_MEMORY
+	BUG_ON(!is_emem_dma(addr));
+#endif
+
+	return addr;
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_avail_addr);
 
 dma_addr_t virtqueue_get_used_addr(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
+	dma_addr_t addr;
 
 	BUG_ON(!vq->we_own_ring);
 
 	if (vq->packed_ring)
-		return vq->packed.device_event_dma_addr;
+		addr = vq->packed.device_event_dma_addr;
+	else
+		addr = vq->split.queue_dma_addr +
+			((char *)vq->split.vring.used -
+			(char *)vq->split.vring.desc);
+
+#ifdef CONFIG_EXT_MEMORY
+	BUG_ON(!is_emem_dma(addr));
+#endif
 
-	return vq->split.queue_dma_addr +
-		((char *)vq->split.vring.used - (char *)vq->split.vring.desc);
+	return addr;
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_used_addr);
 
diff --git a/include/linux/emem.h b/include/linux/emem.h
new file mode 100644
index 000000000000..efc7fd87401e
--- /dev/null
+++ b/include/linux/emem.h
@@ -0,0 +1,63 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __MM_EMEM_H__
+#define __MM_EMEM_H__
+
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <asm/dma.h>
+
+#define EMEM_ALLOC_MAXORDER 15
+#define EMEM_IOSPLIT 2
+
+struct emem_region {
+	/* vmemmap region */
+	unsigned long *bitmap;
+	unsigned int npages;
+	unsigned int bits;
+	phys_addr_t dma_base;
+	u64 base;
+	/* iomapped region */
+	unsigned long *iobitmap;
+	unsigned long iopages;
+	unsigned int iobits;
+	phys_addr_t io_dma_base;
+	u64 iobase;
+};
+
+extern struct emem_region emem;
+extern spinlock_t emem_lock;
+
+#ifdef CONFIG_EXT_MEMORY
+int __init emem_region_init(void);
+int is_emem(void *vaddr);
+int is_emem_dma(dma_addr_t addr);
+int is_emem_io(void *vaddr);
+int is_emem_io_phys(phys_addr_t addr);
+
+/* IO allocations */
+void *emem_getpages_io(gfp_t flags, int order);
+void emem_freepages_io(void *, int order);
+
+/* Kernel memory */
+struct page *emem_getpages_unlocked(gfp_t flags, int order);
+struct page *emem_getpages(gfp_t flags, int order);
+void emem_freepages(struct page *page, int order);
+#else
+static inline int __init emem_region_init(void) { return 0 };
+static inline int is_emem(void *vaddr) { return 0; };
+static inline int is_emem_dma(dma_addr_t addr) { return 0; };
+static inline int is_emem_io(void *vaddr) { return 0; };
+static inline int is_emem_io_phys(phys_addr_t addr) { return 0; };
+static inline struct page *emem_getpages_unlocked(gfp_t flags, int order)
+{
+	return NULL;
+};
+static inline struct page *emem_getpages(gfp_t flags, int order)
+{
+	return NULL;
+};
+static inline void emem_freepages(struct page *page, int order) { };
+static inline void *emem_getpages_io(gfp_t flags, int order) { };
+static inline void emem_freepages_io(void *, int order) { };
+#endif
+#endif // __MM_EMEM_H__
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index 65a78773dcca..fab4e7cc80c7 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -54,6 +54,10 @@ static inline bool gfpflags_allow_blocking(const gfp_t gfp_flags)
 #define OPT_ZONE_DMA32 ZONE_NORMAL
 #endif
 
+#ifdef CONFIG_ZONE_EXT
+#define OPT_ZONE_EXT ZONE_EXT
+#endif
+
 /*
  * GFP_ZONE_TABLE is a word size bitstring that is used for looking up the
  * zone to use given the lowest 4 bits of gfp_t. Entries are GFP_ZONES_SHIFT
diff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h
index d88c46ca82e1..0af63fc82cf1 100644
--- a/include/linux/gfp_types.h
+++ b/include/linux/gfp_types.h
@@ -60,6 +60,12 @@ typedef unsigned int __bitwise gfp_t;
 #else
 #define ___GFP_NOLOCKDEP	0
 #endif
+#ifdef CONFIG_EXT_MEMORY
+#define ___GFP_EXT		0x10000000u
+#else
+#define ___GFP_EXT		0
+#endif
+
 /* If the above are modified, __GFP_BITS_SHIFT may need updating */
 
 /*
@@ -75,6 +81,9 @@ typedef unsigned int __bitwise gfp_t;
 #define __GFP_MOVABLE	((__force gfp_t)___GFP_MOVABLE)  /* ZONE_MOVABLE allowed */
 #define GFP_ZONEMASK	(__GFP_DMA|__GFP_HIGHMEM|__GFP_DMA32|__GFP_MOVABLE)
 
+/* Shared external allocation, non buddy */
+#define __GFP_EXT       ((__force gfp_t)___GFP_EXT)
+
 /**
  * DOC: Page mobility and placement hints
  *
@@ -256,7 +265,7 @@ typedef unsigned int __bitwise gfp_t;
 #define __GFP_NOLOCKDEP ((__force gfp_t)___GFP_NOLOCKDEP)
 
 /* Room for N __GFP_FOO bits */
-#define __GFP_BITS_SHIFT (27 + IS_ENABLED(CONFIG_LOCKDEP))
+#define __GFP_BITS_SHIFT (28 + IS_ENABLED(CONFIG_EXT_MEMORY))
 #define __GFP_BITS_MASK ((__force gfp_t)((1 << __GFP_BITS_SHIFT) - 1))
 
 /**
@@ -313,6 +322,10 @@ typedef unsigned int __bitwise gfp_t;
  * because the DMA32 kmalloc cache array is not implemented.
  * (Reason: there is no such user in kernel).
  *
+ * %GFP_EXT indicates that this allocation should be done from an external
+ * memory pool added via the memory hotplug. This allocation type requires
+ * separate configuration and cannot be used without it.
+ *
  * %GFP_HIGHUSER is for userspace allocations that may be mapped to userspace,
  * do not need to be directly accessible by the kernel but that cannot
  * move once in use. An example may be a hardware allocation that maps
@@ -338,6 +351,7 @@ typedef unsigned int __bitwise gfp_t;
 #define GFP_USER	(__GFP_RECLAIM | __GFP_IO | __GFP_FS | __GFP_HARDWALL)
 #define GFP_DMA		__GFP_DMA
 #define GFP_DMA32	__GFP_DMA32
+#define GFP_EXT		__GFP_EXT
 #define GFP_HIGHUSER	(GFP_USER | __GFP_HIGHMEM)
 #define GFP_HIGHUSER_MOVABLE	(GFP_HIGHUSER | __GFP_MOVABLE | \
 			 __GFP_SKIP_KASAN_POISON | __GFP_SKIP_KASAN_UNPOISON)
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 45efc6c553b8..9bcfbf944f2e 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -114,6 +114,12 @@
 #define SLAB_KASAN		0
 #endif
 
+#ifdef CONFIG_EXT_MEMORY
+#define SLAB_CACHE_EXT		((slab_flags_t __force)0x20000000U)
+#else
+#define SLAB_CACHE_EXT		0
+#endif
+
 /*
  * Ignore user specified debugging flags.
  * Intended for caches created for self-tests so they have only flags
@@ -149,6 +155,7 @@
 
 struct list_lru;
 struct mem_cgroup;
+
 /*
  * struct kmem_cache related prototypes
  */
@@ -338,6 +345,11 @@ enum kmalloc_cache_type {
 	KMALLOC_CGROUP = KMALLOC_NORMAL,
 #else
 	KMALLOC_CGROUP,
+#endif
+#ifdef CONFIG_ZONE_EXT
+	KMALLOC_EXT,
+#else
+	KMALLOC_EXT = KMALLOC_NORMAL,
 #endif
 	KMALLOC_RECLAIM,
 #ifdef CONFIG_ZONE_DMA
@@ -356,7 +368,8 @@ kmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1];
 #define KMALLOC_NOT_NORMAL_BITS					\
 	(__GFP_RECLAIMABLE |					\
 	(IS_ENABLED(CONFIG_ZONE_DMA)   ? __GFP_DMA : 0) |	\
-	(IS_ENABLED(CONFIG_MEMCG_KMEM) ? __GFP_ACCOUNT : 0))
+	(IS_ENABLED(CONFIG_MEMCG_KMEM) ? __GFP_ACCOUNT : 0) |	\
+	(IS_ENABLED(CONFIG_EXT_MEMORY) ? __GFP_EXT : 0))
 
 static __always_inline enum kmalloc_cache_type kmalloc_type(gfp_t flags)
 {
@@ -371,11 +384,14 @@ static __always_inline enum kmalloc_cache_type kmalloc_type(gfp_t flags)
 	 * At least one of the flags has to be set. Their priorities in
 	 * decreasing order are:
 	 *  1) __GFP_DMA
-	 *  2) __GFP_RECLAIMABLE
-	 *  3) __GFP_ACCOUNT
+	 *  2) __GFP_EXT
+	 *  3) __GFP_RECLAIMABLE
+	 *  4) __GFP_ACCOUNT
 	 */
 	if (IS_ENABLED(CONFIG_ZONE_DMA) && (flags & __GFP_DMA))
 		return KMALLOC_DMA;
+	if (IS_ENABLED(CONFIG_ZONE_EXT) && (flags & __GFP_EXT))
+		return KMALLOC_EXT;
 	if (!IS_ENABLED(CONFIG_MEMCG_KMEM) || (flags & __GFP_RECLAIMABLE))
 		return KMALLOC_RECLAIM;
 	else
@@ -535,6 +551,10 @@ void *kmalloc_large_node(size_t size, gfp_t flags, int node) __assume_page_align
  * %__GFP_RETRY_MAYFAIL
  *	Try really hard to succeed the allocation but fail
  *	eventually.
+ *
+ * %__GFP_EXT
+ *	This allocation should be done from an external memory pool.
+ *
  */
 static __always_inline __alloc_size(1) void *kmalloc(size_t size, gfp_t flags)
 {
diff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h
index e87cb2b80ed3..41b52e990051 100644
--- a/include/trace/events/mmflags.h
+++ b/include/trace/events/mmflags.h
@@ -30,6 +30,7 @@
 	gfpflag_string(GFP_DMA),		\
 	gfpflag_string(__GFP_HIGHMEM),		\
 	gfpflag_string(GFP_DMA32),		\
+	gfpflag_string(__GFP_EXT),		\
 	gfpflag_string(__GFP_HIGH),		\
 	gfpflag_string(__GFP_ATOMIC),		\
 	gfpflag_string(__GFP_IO),		\
@@ -51,13 +52,13 @@
 	gfpflag_string(__GFP_RECLAIM),		\
 	gfpflag_string(__GFP_DIRECT_RECLAIM),	\
 	gfpflag_string(__GFP_KSWAPD_RECLAIM),	\
-	gfpflag_string(__GFP_ZEROTAGS)
+	gfpflag_string(__GFP_ZEROTAGS)		\
 
 #ifdef CONFIG_KASAN_HW_TAGS
 #define __def_gfpflag_names_kasan ,			\
 	gfpflag_string(__GFP_SKIP_ZERO),		\
 	gfpflag_string(__GFP_SKIP_KASAN_POISON),	\
-	gfpflag_string(__GFP_SKIP_KASAN_UNPOISON)
+	gfpflag_string(__GFP_SKIP_KASAN_UNPOISON)	\
 #else
 #define __def_gfpflag_names_kasan
 #endif
diff --git a/init/main.c b/init/main.c
index aa21add5f7c5..6a1925189349 100644
--- a/init/main.c
+++ b/init/main.c
@@ -101,6 +101,8 @@
 #include <linux/init_syscalls.h>
 #include <linux/stackdepot.h>
 #include <linux/randomize_kstack.h>
+#include <linux/swiotlb.h>
+#include <linux/emem.h>
 #include <net/net_namespace.h>
 
 #include <asm/io.h>
@@ -1035,6 +1037,13 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 	if (initcall_debug)
 		initcall_debug_enable();
 
+	/*
+	 * Needs to happen before the irq init as we may need to share
+	 * some memory for the irq controller itself.
+	 */
+	emem_region_init();
+	swiotlb_init(max_pfn > PFN_DOWN(arm64_dma_phys_limit), SWIOTLB_VERBOSE);
+
 	context_tracking_init();
 	/* init some links before init_ISA_irqs() */
 	early_irq_init();
diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index 6ea80ae42622..29abfe8c1c33 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -50,6 +50,7 @@
 #include <linux/sizes.h>
 #include <linux/dma-map-ops.h>
 #include <linux/cma.h>
+#include <linux/emem.h>
 
 #ifdef CONFIG_CMA_SIZE_MBYTES
 #define CMA_SIZE_MBYTES CONFIG_CMA_SIZE_MBYTES
@@ -303,10 +304,27 @@ static struct page *cma_alloc_aligned(struct cma *cma, size_t size, gfp_t gfp)
  */
 struct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp)
 {
+	struct page *page;
+
 #ifdef CONFIG_DMA_PERNUMA_CMA
 	int nid = dev_to_node(dev);
 #endif
 
+	if (unlikely(gfp & __GFP_EXT)) {
+		if (gfp & GFP_DMA32)
+			pr_warn("DMA32 request for emem, check your config!\n");
+
+		page = emem_getpages(gfp, get_order(round_up(size, PAGE_SIZE)));
+		if (!page) {
+			pr_err("emem_getpages out of memory?\n");
+			return NULL;
+		}
+		if (PTR_ERR(page) == -ENOTSUPP)
+			panic("attempting to allocate emem too early?\n");
+
+		return page;
+	}
+
 	/* CMA can be used only in the context which permits sleeping */
 	if (!gfpflags_allow_blocking(gfp))
 		return NULL;
diff --git a/kernel/dma/mapping.c b/kernel/dma/mapping.c
index 33437d620644..b0aeea7073e9 100644
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@ -14,6 +14,7 @@
 #include <linux/of_device.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
+#include <linux/emem.h>
 #include "debug.h"
 #include "direct.h"
 
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 7f4ad5e70b40..a33843cd1929 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -41,6 +41,7 @@
 #include <linux/string.h>
 #include <linux/swiotlb.h>
 #include <linux/types.h>
+#include <linux/emem.h>
 #ifdef CONFIG_DMA_RESTRICTED_POOL
 #include <linux/of.h>
 #include <linux/of_fdt.h>
@@ -308,6 +309,7 @@ void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 		int (*remap)(void *tlb, unsigned long nslabs))
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+	struct page *page;
 	unsigned long nslabs;
 	size_t alloc_size;
 	size_t bytes;
@@ -333,17 +335,15 @@ void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 	 */
 retry:
 	bytes = PAGE_ALIGN(nslabs << IO_TLB_SHIFT);
-	if (flags & SWIOTLB_ANY)
-		tlb = memblock_alloc(bytes, PAGE_SIZE);
-	else
-		tlb = memblock_alloc_low(bytes, PAGE_SIZE);
-	if (!tlb) {
-		pr_warn("%s: failed to allocate tlb structure\n", __func__);
+	page = emem_getpages(GFP_KERNEL, get_order(round_up(bytes, PAGE_SIZE)));
+	if (!page || (PTR_ERR(page) == -ENOTSUPP)) {
+		panic("%s: failed to allocate tlb structure\n", __func__);
 		return;
 	}
+	tlb = page_to_virt(page);
 
 	if (remap && remap(tlb, nslabs) < 0) {
-		memblock_free(tlb, PAGE_ALIGN(bytes));
+		emem_freepages(tlb, get_order(round_up(bytes, PAGE_SIZE)));
 
 		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
 		if (nslabs >= IO_TLB_MIN_SLABS)
@@ -354,25 +354,26 @@ void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 	}
 
 	alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
-	mem->slots = memblock_alloc(alloc_size, PAGE_SIZE);
-	if (!mem->slots) {
-		pr_warn("%s: Failed to allocate %zu bytes align=0x%lx\n",
-			__func__, alloc_size, PAGE_SIZE);
+	page = emem_getpages(GFP_KERNEL, get_order(round_up(alloc_size, PAGE_SIZE)));
+	if (!page || (PTR_ERR(page) == -ENOTSUPP)) {
+		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
+		      __func__, alloc_size, PAGE_SIZE);
 		return;
 	}
+	mem->slots = page_to_virt(page);
 
-	mem->areas = memblock_alloc(array_size(sizeof(struct io_tlb_area),
-		default_nareas), SMP_CACHE_BYTES);
-	if (!mem->areas) {
-		pr_warn("%s: Failed to allocate mem->areas.\n", __func__);
+	page = emem_getpages(GFP_KERNEL,
+			get_order(round_up(array_size(sizeof(struct io_tlb_area),
+			default_nareas), PAGE_SIZE)));
+	if (!page || (PTR_ERR(page) == -ENOTSUPP)) {
+		panic("%s: Failed to allocate mem->areas.\n", __func__);
 		return;
 	}
-
+	mem->areas = page_to_virt(page);
 	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, flags, false,
 				default_nareas);
 
-	if (flags & SWIOTLB_VERBOSE)
-		swiotlb_print_info();
+	swiotlb_print_info();
 }
 
 void __init swiotlb_init(bool addressing_limit, unsigned int flags)
@@ -625,6 +626,7 @@ static int swiotlb_do_find_slots(struct device *dev, int area_index,
 
 	BUG_ON(!nslots);
 	BUG_ON(area_index >= mem->nareas);
+	BUG_ON(!is_emem_dma((dma_addr_t)mem->start));
 
 	/*
 	 * For mappings with an alignment requirement don't bother looping to
diff --git a/mm/Kconfig b/mm/Kconfig
index 57e1d8c5b505..89f00a9f41f2 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -958,6 +958,10 @@ config ZONE_DMA32
 	depends on !X86_32
 	default y if ARM64
 
+config ZONE_EXT
+	bool "Support external allocation pool"
+	depends on MEMORY_HOTPLUG
+
 config ZONE_DEVICE
 	bool "Device memory (pmem, HMM, etc...) hotplug support"
 	depends on MEMORY_HOTPLUG
@@ -975,6 +979,17 @@ config ZONE_DEVICE
 
 	  If FS_DAX is enabled, then say Y.
 
+config EXT_MEMORY
+	bool "Support for allocations from a external memory pool"
+	select ZONE_EXT
+
+	help
+	  Support for kernel memory allocations outside of the regular
+	  kernel regions. These regions are configured separately via
+	  the device tree.
+
+	  If unsure, say N.
+
 #
 # Helpers to mirror range of the CPU page tables of a process into device page
 # tables.
diff --git a/mm/Makefile b/mm/Makefile
index 8e105e5b3e29..75d99bf588ef 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -138,3 +138,4 @@ obj-$(CONFIG_IO_MAPPING) += io-mapping.o
 obj-$(CONFIG_HAVE_BOOTMEM_INFO_NODE) += bootmem_info.o
 obj-$(CONFIG_GENERIC_IOREMAP) += ioremap.o
 obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o
+obj-$(CONFIG_EXT_MEMORY) += emem.o
diff --git a/mm/emem.c b/mm/emem.c
new file mode 100644
index 000000000000..151ab30b1bfb
--- /dev/null
+++ b/mm/emem.c
@@ -0,0 +1,426 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *  linux/mm/emem.c
+ */
+
+#include <linux/slab.h>
+#include <linux/memory_hotplug.h>
+#include <linux/of_address.h>
+#include <linux/of.h>
+#include <linux/emem.h>
+#include <linux/memblock.h>
+
+#define EMEM_NAME "emem"
+
+const bool emem_host = false;
+extern gfp_t gfp_allowed_mask;
+struct emem_region emem;
+DEFINE_SPINLOCK(emem_lock);
+static bool emem_init_done;
+
+static int emem_major;
+static atomic_t emem_dopen = ATOMIC_INIT(0);
+
+int emem_open(struct inode *inode, struct file *file)
+{
+	return atomic_add_unless(&emem_dopen, 1, 1) ? 0 : -EBUSY;
+}
+
+int emem_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	int res;
+
+	vma->vm_flags |= VM_READ | VM_WRITE;
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	res = vm_iomap_memory(vma, emem.dma_base,
+			      emem.npages * PAGE_SIZE);
+	if (res) {
+		pr_err("emem: io_remap_memory(0x%llx,%lu) returned %d\n",
+			emem.dma_base, emem.npages * PAGE_SIZE, res);
+	}
+	return res;
+}
+
+int emem_release(struct inode *inode, struct file *file)
+{
+	iounmap((void *)emem.dma_base);
+	atomic_set(&emem_dopen, 0);
+	return 0;
+}
+
+long emem_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	return -ENOTSUPP;
+}
+
+static const struct file_operations fops = {
+	.mmap = emem_mmap,
+	.open = emem_open,
+	.release = emem_release,
+	.mmap = emem_mmap,
+	.unlocked_ioctl = emem_ioctl,
+};
+
+static int emem_device_init(void)
+{
+	if (emem_major)
+		return 0;
+
+	emem_major = register_chrdev(0, EMEM_NAME, &fops);
+	if (emem_major < 0) {
+		pr_err("emem: register_chrdev failed with %d\n",
+			emem_major);
+		return emem_major;
+	}
+
+	pr_info("emem: mknod /dev/%s c %d 0\n", EMEM_NAME, emem_major);
+	return 0;
+}
+late_initcall(emem_device_init);
+
+static void print_region(u64 start_addr, u64 end_addr)
+{
+	pr_info("emem: region 0x%llx - 0x%llx, pages 0x%llx - 0x%llx\n",
+		start_addr, end_addr,
+		(u64)virt_to_page(start_addr),
+		(u64)virt_to_page(end_addr));
+}
+
+static int emem_get_config(struct device_node *np, phys_addr_t *base, size_t *size)
+{
+	struct device_node *shm_np;
+	struct resource res_mem;
+	int ret;
+
+	shm_np = of_parse_phandle(np, "memory-region", 0);
+	if (!shm_np)
+		return -EINVAL;
+
+	ret = of_address_to_resource(shm_np, 0, &res_mem);
+	if (ret)
+		return -EINVAL;
+
+	*base = res_mem.start;
+	*size = resource_size(&res_mem);
+	of_node_put(shm_np);
+
+	if (!*base || !*size)
+		return -EINVAL;
+
+	pr_info("emem: shm base at 0x%llx size %lu\n", *base, *size);
+	return 0;
+}
+
+__attribute__((optimize(0)))
+int emem_region_init(void)
+{
+	struct device_node *np;
+	phys_addr_t base;
+	unsigned long iflags;
+	size_t size, iosize;
+	int res, ret = 0;
+	void *io;
+
+	if (emem_init_done)
+		return 0;
+	emem_init_done = true;
+
+	spin_lock_irqsave(&emem_lock, iflags);
+	np = of_find_node_by_path("/emem_region");
+	if (!np) {
+		pr_err("emem: region not configured\n");
+		res = -ENOTSUPP;
+		goto out;
+	}
+
+	res = emem_get_config(np, &base, &size);
+	of_node_put(np);
+	if (res)
+		goto out;
+
+	if ((base % PAGE_SIZE) || (size % PAGE_SIZE) || !size) {
+		res = -EINVAL;
+		goto out;
+	}
+
+	/* Add the block and make sure it's not part of existing zones */
+	if (!emem_host)
+		iosize = size / EMEM_IOSPLIT;
+	else
+		iosize = 0;
+
+retry:
+	res = add_memory(0, base, size - iosize, MMOP_OFFLINE);
+	if ((res == -EEXIST) && !ret) {
+		pr_warn("emem: memory block already exists, trying to offline it\n");
+		ret = 1;
+		res = remove_memory(base, size - iosize);
+		if (res == 0)
+			goto retry;
+		else
+			goto out;
+	} else if (res != 0)
+		goto out;
+
+	emem.dma_base = base;
+	emem.npages = (size - iosize) / PAGE_SIZE;
+
+	/* Host does not allocate, just accesses */
+	if (emem_host)
+		goto out;
+
+	/* Kernel memory setup */
+	emem.bitmap = kzalloc(GFP_KERNEL | GFP_ATOMIC, emem.npages * 8);
+	if (!emem.bitmap) {
+		res = -ENOMEM;
+		goto out;
+	}
+	emem.bits = emem.npages * PAGE_SIZE * 8;
+	emem.base = (u64)phys_to_virt(emem.dma_base);
+
+	/* IO memory setup */
+	emem.io_dma_base = emem.dma_base + (emem.npages * PAGE_SIZE);
+	io = ioremap(emem.io_dma_base, iosize);
+	if (!io)
+		panic("unable to ioremap io block\n");
+	emem.iobase = (u64)io;
+	emem.iopages = iosize / PAGE_SIZE;
+	emem.iobits = iosize * 8;
+	emem.iobitmap = kzalloc(GFP_KERNEL | GFP_ATOMIC, (emem.iobits / 8));
+	if (!emem.iobitmap) {
+		res = -ENOMEM;
+		goto out;
+	}
+
+	/* Now allow allocations into this region */
+	gfp_allowed_mask |= GFP_EXT;
+
+out:
+	if (res == 0) {
+		if (emem_host)
+			pr_info("emem: host enabled for 0x%llx - 0x%llx\n",
+				(u64)base, (u64)(base + size));
+		else
+			pr_info("emem: guest region at 0x%llx - 0x%llx\n",
+				(u64)base, (u64)(base + size));
+	} else
+		pr_err("emem: region registration failed, error %d\n", res);
+
+	spin_unlock_irqrestore(&emem_lock, iflags);
+	return res;
+}
+
+int is_emem(void *vaddr)
+{
+	u64 s, e, v = (u64)vaddr;
+
+	s = emem.base;
+	e = s + (emem.npages * PAGE_SIZE);
+	if ((v >= s) && (v < e))
+		return 1;
+	return 0;
+}
+EXPORT_SYMBOL(is_emem);
+
+int is_emem_dma(dma_addr_t addr)
+{
+	u64 s, e, a = (u64)addr;
+
+	s = (u64)emem.dma_base;
+	e = s + (emem.npages * PAGE_SIZE);
+	if ((a >= s) && (a < e))
+		return 1;
+	return 0;
+}
+EXPORT_SYMBOL(is_emem_dma);
+
+int is_emem_io(void *vaddr)
+{
+	u64 s, e, v = (u64)vaddr;
+
+	s = emem.iobase;
+	e = s + (emem.iopages * PAGE_SIZE);
+	if ((v >= s) && (v < e))
+		return 1;
+	return 0;
+}
+EXPORT_SYMBOL(is_emem_io);
+
+int is_emem_io_phys(phys_addr_t addr)
+{
+	u64 s, e, v = (u64)addr;
+
+	s = emem.io_dma_base;
+	e = s + (emem.iopages * PAGE_SIZE);
+	if ((v >= s) && (v < e))
+		return 1;
+	return 0;
+}
+EXPORT_SYMBOL(is_emem_io_phys);
+
+struct page *emem_getpages_unlocked(gfp_t flags, int order)
+{
+	struct page *page, *npage;
+	void *vaddr = NULL, *eaddr;
+	u64 start_addr, end_addr;
+	int pageno;
+
+	pageno = bitmap_find_free_region(emem.bitmap, emem.bits, order);
+	if (pageno >= 0)
+		vaddr = (void *)emem.base + (pageno << PAGE_SHIFT);
+	if (!vaddr)
+		return NULL;
+
+	eaddr = vaddr + ((1 << order) * PAGE_SIZE);
+	page = virt_to_page(vaddr);
+	start_addr = 0;
+	end_addr = 0;
+
+	while (vaddr < eaddr) {
+		npage = virt_to_page(vaddr);
+		if (!npage)
+			panic("emem: no page for 0x%llx?\n", (u64)vaddr);
+
+		get_page(npage);
+		SetPageReserved(npage);
+
+		/* This is only for debugging */
+		if (page_ref_count(npage) > 1) {
+			if (!start_addr)
+				start_addr = (u64)vaddr;
+			end_addr = (u64)vaddr;
+		} else {
+			if (start_addr)
+				print_region(start_addr, end_addr);
+			start_addr = 0;
+			end_addr = 0;
+		}
+		if (flags & __GFP_ZERO)
+			memset(vaddr, 0, PAGE_SIZE);
+
+		vaddr += PAGE_SIZE;
+	}
+	pr_info("emem: allocated kmem 0x%llx - 0x%llx\n",
+		(u64)page_to_virt(page), (u64)eaddr - 1);
+
+	return page;
+}
+EXPORT_SYMBOL_GPL(emem_getpages_unlocked);
+
+struct page *emem_getpages(gfp_t flags, int order)
+{
+	struct page *page;
+	unsigned long iflags;
+
+	if (order > EMEM_ALLOC_MAXORDER)
+		return NULL;
+
+	if (!emem.bitmap)
+		return ERR_PTR(-ENOTSUPP);
+
+	spin_lock_irqsave(&emem_lock, iflags);
+	page = emem_getpages_unlocked(flags, order);
+	spin_unlock_irqrestore(&emem_lock, iflags);
+
+	return page;
+}
+EXPORT_SYMBOL_GPL(emem_getpages);
+
+void emem_freepages(struct page *page, int order)
+{
+	unsigned long irq_flags;
+	void *vaddr, *eaddr;
+	int pageno;
+
+	if (!emem.bitmap || !page)
+		return;
+
+	if (order > EMEM_ALLOC_MAXORDER) {
+		pr_crit("emem: free of insane order %d\n", order);
+		return;
+	}
+
+	vaddr = page_to_virt(page);
+	if (!is_emem(vaddr)) {
+		pr_crit("emem: attempting to free non-emem page at 0x%llx\n",
+			(u64)vaddr);
+		return;
+	}
+	pageno = (vaddr - (void *)emem.base) >> PAGE_SHIFT;
+
+	spin_lock_irqsave(&emem_lock, irq_flags);
+	eaddr = vaddr + ((1 << order) * PAGE_SIZE);
+	while (vaddr < eaddr) {
+		page = virt_to_page(vaddr);
+		if (!page)
+			panic("emem: no page\n");
+
+		page_mapcount_reset(page);
+		ClearPageReserved(page);
+
+		if (page_ref_count(page) > 1)
+			panic("emem: page 0x%llx still in use?\n", (u64)vaddr);
+
+		put_page(page);
+		vaddr += PAGE_SIZE;
+	}
+	/* Release it all */
+	bitmap_release_region(emem.bitmap, pageno, order);
+	spin_unlock_irqrestore(&emem_lock, irq_flags);
+
+	pr_info("emem: freed kmem 0x%llx - 0x%llx\n",
+		(u64)page_to_virt(page),(u64)eaddr - 1);
+}
+EXPORT_SYMBOL_GPL(emem_freepages);
+
+void *emem_getpages_io(gfp_t flags, int order)
+{
+	unsigned long irq_flags;
+	void *vaddr = NULL;
+	int pageno;
+
+	if (order > EMEM_ALLOC_MAXORDER)
+		return NULL;
+
+	if (!emem.iobitmap)
+		return ERR_PTR(-ENOTSUPP);
+
+	if (!(flags & GFP_DMA))
+		return ERR_PTR(-ENOTSUPP);
+
+	spin_lock_irqsave(&emem_lock, irq_flags);
+	pageno = bitmap_find_free_region(emem.iobitmap, emem.iobits, order);
+	if (pageno >= 0)
+		vaddr = (void *)emem.iobase + (pageno << PAGE_SHIFT);
+	spin_unlock_irqrestore(&emem_lock, irq_flags);
+
+	if ((pageno >= 0) && (flags & __GFP_ZERO))
+		memset(vaddr, 0, (1 << order) * PAGE_SIZE);
+
+	pr_info("emem: allocated iomem 0x%llx - 0x%llx for IO\n", (u64)vaddr,
+		(u64)(vaddr + ((1 << order) * PAGE_SIZE) -1));
+
+	return vaddr;
+}
+EXPORT_SYMBOL_GPL(emem_getpages_io);
+
+void emem_freepages_io(void *vaddr, int order)
+{
+	unsigned long irq_flags;
+	int pageno;
+
+	if (order > EMEM_ALLOC_MAXORDER) {
+			pr_crit("emem: free of insane order %d\n", order);
+			return;
+	}
+	pageno = (vaddr - (void *)emem.iobase) >> PAGE_SHIFT;
+
+	spin_lock_irqsave(&emem_lock, irq_flags);
+	bitmap_release_region(emem.iobitmap, pageno, order);
+	spin_unlock_irqrestore(&emem_lock, irq_flags);
+
+	pr_info("emem: freed iomem 0x%llx - 0x%llx\n", (u64)vaddr,
+		(u64)(vaddr + ((1 << order) * PAGE_SIZE) -1));
+}
+EXPORT_SYMBOL_GPL(emem_freepages_io);
diff --git a/mm/mempool.c b/mm/mempool.c
index 96488b13a1ef..2c8e71cb3418 100644
--- a/mm/mempool.c
+++ b/mm/mempool.c
@@ -384,6 +384,7 @@ void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 	gfp_mask |= __GFP_NOMEMALLOC;	/* don't allocate emergency reserves */
 	gfp_mask |= __GFP_NORETRY;	/* don't loop in __alloc_pages */
 	gfp_mask |= __GFP_NOWARN;	/* failures are OK */
+	gfp_mask |= __GFP_EXT;		/* if enabled, used it */
 
 	gfp_temp = gfp_mask & ~(__GFP_DIRECT_RECLAIM|__GFP_IO);
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 69668817fed3..7eeeab7f9195 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -76,6 +76,7 @@
 #include <linux/khugepaged.h>
 #include <linux/buffer_head.h>
 #include <linux/delayacct.h>
+#include <linux/emem.h>
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -5536,6 +5537,22 @@ struct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,
 		return NULL;
 
 	gfp &= gfp_allowed_mask;
+
+	if (unlikely(gfp & __GFP_EXT)) {
+		if (gfp & GFP_DMA32)
+			pr_warn("DMA32 request for emem, check your config!\n");
+
+		page = emem_getpages(gfp, order);
+		if (!page) {
+			pr_err("emem_getpages out of memory?\n");
+			return NULL;
+		}
+		if (PTR_ERR(page) == -ENOTSUPP)
+			panic("attempting to allocate emem too early?\n");
+
+		goto out;
+	}
+
 	/*
 	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
 	 * resp. GFP_NOIO which has to be inherited for all allocation requests
@@ -5641,14 +5658,24 @@ EXPORT_SYMBOL(get_zeroed_page);
  */
 void __free_pages(struct page *page, unsigned int order)
 {
+	struct page *p;
+
 	/* get PageHead before we drop reference */
 	int head = PageHead(page);
 
-	if (put_page_testzero(page))
-		free_the_page(page, order);
-	else if (!head)
-		while (order-- > 0)
-			free_the_page(page + (1 << order), order);
+	if (put_page_testzero(page)) {
+		if (unlikely(is_emem(page_to_virt(page))))
+			emem_freepages(page, order);
+		else
+			free_the_page(page, order);
+	} else if (!head)
+		while (order-- > 0) {
+			p = page + (1 << order);
+			if (unlikely(is_emem(page_to_virt(p))))
+				emem_freepages(p, order);
+			else
+				free_the_page(p, order);
+		}
 }
 EXPORT_SYMBOL(__free_pages);
 
diff --git a/mm/slab_common.c b/mm/slab_common.c
index 0042fb2730d1..b83d60cb3015 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -770,6 +770,7 @@ EXPORT_SYMBOL(kmalloc_size_roundup);
 {								\
 	.name[KMALLOC_NORMAL]  = "kmalloc-" #__short_size,	\
 	.name[KMALLOC_RECLAIM] = "kmalloc-rcl-" #__short_size,	\
+	.name[KMALLOC_EXT]     = "kmalloc-ext-" #__short_size,  \
 	KMALLOC_CGROUP_NAME(__short_size)			\
 	KMALLOC_DMA_NAME(__short_size)				\
 	.size = __size,						\
@@ -866,6 +867,8 @@ new_kmalloc_cache(int idx, enum kmalloc_cache_type type, slab_flags_t flags)
 	} else if (IS_ENABLED(CONFIG_ZONE_DMA) && (type == KMALLOC_DMA)) {
 		flags |= SLAB_CACHE_DMA;
 	}
+	if (type == KMALLOC_EXT)
+		flags |= SLAB_CACHE_EXT;
 
 	kmalloc_caches[type][idx] = create_kmalloc_cache(
 					kmalloc_info[idx].name[type],
diff --git a/mm/slub.c b/mm/slub.c
index 157527d7101b..98858a0ba145 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1928,6 +1928,14 @@ static struct slab *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	flags |= s->allocflags;
 
+	/* Mandatory ext adjustments */
+	if (flags & __GFP_EXT) {
+		if (flags & __GFP_MOVABLE)
+			flags &= ~__GFP_MOVABLE;
+		if (flags & __GFP_RECLAIMABLE)
+			flags &= ~__GFP_RECLAIMABLE;
+	}
+
 	/*
 	 * Let the initial higher-order allocation fail under memory pressure
 	 * so we fall-back to the minimum order allocation.
@@ -4285,6 +4293,9 @@ static int calculate_sizes(struct kmem_cache *s)
 	if (s->flags & SLAB_CACHE_DMA32)
 		s->allocflags |= GFP_DMA32;
 
+	if (s->flags & SLAB_CACHE_EXT)
+		s->allocflags |= GFP_EXT;
+
 	if (s->flags & SLAB_RECLAIM_ACCOUNT)
 		s->allocflags |= __GFP_RECLAIMABLE;
 
@@ -5895,6 +5906,8 @@ static char *create_unique_id(struct kmem_cache *s)
 		*p++ = 'd';
 	if (s->flags & SLAB_CACHE_DMA32)
 		*p++ = 'D';
+	if (s->flags & SLAB_CACHE_EXT)
+		*p++ = 'E';
 	if (s->flags & SLAB_RECLAIM_ACCOUNT)
 		*p++ = 'a';
 	if (s->flags & SLAB_CONSISTENCY_CHECKS)
diff --git a/mm/sparse.c b/mm/sparse.c
index e5a8a3a0edd7..b2459501c1db 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -909,6 +909,7 @@ int __meminit sparse_add_section(int nid, unsigned long start_pfn,
 	 * combinations.
 	 */
 	page_init_poison(memmap, sizeof(struct page) * nr_pages);
+	pr_err("ALLOCATED VMEMMAP FOR PFN 0x%lx AT 0x%llx, NO_PAGES %lu\n", start_pfn, (u64)memmap, nr_pages);
 
 	ms = __nr_to_section(section_nr);
 	set_section_nid(section_nr, nid);
diff --git a/tools/perf/builtin-kmem.c b/tools/perf/builtin-kmem.c
index 40dd52acc48a..359da8a403e3 100644
--- a/tools/perf/builtin-kmem.c
+++ b/tools/perf/builtin-kmem.c
@@ -672,6 +672,7 @@ static const struct {
 	{ "__GFP_RECLAIM",		"R" },
 	{ "__GFP_DIRECT_RECLAIM",	"DR" },
 	{ "__GFP_KSWAPD_RECLAIM",	"KR" },
+	{ "__GFP_EXT",			"EXT" },
 };
 
 static size_t max_gfp_len;
-- 
2.34.1

